{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e0dad4",
   "metadata": {},
   "source": [
    "# Technical Challenge - Code Review and Deployment Pipeline Orchestration\n",
    "\n",
    "**Format:** Structured interview with whiteboarding/documentation  \n",
    "**Assessment Focus:** Problem decomposition, AI prompting strategy, system design\n",
    "\n",
    "**Please Fill in your Responses in the Response markdown boxes**\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge Scenario\n",
    "\n",
    "You are tasked with creating an AI-powered system that can handle the complete lifecycle of code review and deployment pipeline management for a mid-size software company. The system needs to:\n",
    "\n",
    "**Current Pain Points:**\n",
    "- Manual code reviews take 2-3 days per PR\n",
    "- Inconsistent review quality across teams\n",
    "- Deployment failures due to missed edge cases\n",
    "- Security vulnerabilities slip through reviews\n",
    "- No standardized deployment process across projects\n",
    "- Rollback decisions are manual and slow\n",
    "\n",
    "**Business Requirements:**\n",
    "- Reduce review time to <4 hours for standard PRs\n",
    "- Maintain or improve code quality\n",
    "- Catch 90%+ of security vulnerabilities before deployment\n",
    "- Standardize deployment across 50+ microservices\n",
    "- Enable automatic rollback based on metrics\n",
    "- Support multiple environments (dev, staging, prod)\n",
    "- Handle both new features and hotfixes\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be761411",
   "metadata": {},
   "source": [
    "## Part A: Problem Decomposition (25 points)\n",
    "\n",
    "**Question 1.1:** Break this challenge down into discrete, manageable steps that could be handled by AI agents or automated systems. Each step should have:\n",
    "- Clear input requirements\n",
    "- Specific output format\n",
    "- Success criteria\n",
    "- Failure handling strategy\n",
    "\n",
    "**Question 1.2:** Which steps can run in parallel? Which are blocking? Where are the critical decision points?\n",
    "\n",
    "**Question 1.3:** Identify the key handoff points between steps. What data/context needs to be passed between each phase?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0a3c10",
   "metadata": {},
   "source": "## Response Part A:\n\n### Question 1.1: Discrete Steps Breakdown\n\n#### **Step 1: PR Intake & Initial Analysis**\n- **Input:** PR metadata (title, description, changed files, diff), repository context\n- **Output:** Structured PR analysis (complexity score, affected components, risk level, estimated review time)\n- **Success Criteria:** Accurate categorization (95%+ accuracy), completion in <30 seconds\n- **Failure Handling:** Flag for manual triage if complexity detection fails; default to \"high complexity\" for safety\n\n#### **Step 2: Static Code Analysis**\n- **Input:** Changed files with context (before/after), language/framework info\n- **Output:** Linting violations, code style issues, potential bugs (JSON format with severity levels)\n- **Success Criteria:** Zero false negatives on critical issues, <10% false positive rate\n- **Failure Handling:** Run fallback linters if primary fails; report partial results with warnings\n\n#### **Step 3: Security Vulnerability Scan**\n- **Input:** Code diff, dependency changes, secrets/credentials detection\n- **Output:** Security findings categorized by severity (CRITICAL, HIGH, MEDIUM, LOW) with remediation suggestions\n- **Success Criteria:** 90%+ vulnerability detection rate, <5% false positives\n- **Failure Handling:** Escalate to security team if scanner errors; block deployment for scanner failures\n\n#### **Step 4: AI Code Review**\n- **Input:** PR context, static analysis results, security scan results, coding standards doc\n- **Output:** Structured review comments (readability, maintainability, design patterns, edge cases) with line-specific annotations\n- **Success Criteria:** Feedback quality rated 4+/5 by developers, catches 80%+ of human-found issues\n- **Failure Handling:** Use template-based review if AI fails; flag for human review\n\n#### **Step 5: Test Coverage Analysis**\n- **Input:** Code changes, existing test suite, coverage reports\n- **Output:** Coverage metrics, missing test scenarios, recommendations for new tests\n- **Success Criteria:** Identify 90%+ of untested code paths, suggest relevant test cases\n- **Failure Handling:** Require manual test review if analysis fails; warn about coverage gaps\n\n#### **Step 6: Build & Integration Testing**\n- **Input:** Merged code (preview), test suite, integration test configs\n- **Output:** Build status, test results, integration test outcomes, performance benchmarks\n- **Success Criteria:** All tests pass, no performance degradation >10%\n- **Failure Handling:** Detailed failure logs, auto-rollback on critical failures, notification to developers\n\n#### **Step 7: Deployment Decision & Orchestration**\n- **Input:** All previous step results, deployment target (dev/staging/prod), business priority\n- **Output:** Go/No-go decision, deployment plan, rollback strategy\n- **Success Criteria:** Correct deployment decisions (validated against historical data), zero unplanned prod incidents\n- **Failure Handling:** Require manual approval if confidence <80%; default to conservative decision\n\n#### **Step 8: Automated Deployment**\n- **Input:** Approved deployment plan, target environment, health check configs\n- **Output:** Deployment status, health metrics, rollout progress\n- **Success Criteria:** Successful deployment in <15 min, zero downtime for blue-green deployments\n- **Failure Handling:** Automatic rollback if health checks fail; alert on-call engineer\n\n#### **Step 9: Post-Deployment Monitoring**\n- **Input:** Deployment metadata, baseline metrics, alert thresholds\n- **Output:** Real-time health status, anomaly detection, performance trends\n- **Success Criteria:** Detect issues within 2 minutes, <1% false positive alert rate\n- **Failure Handling:** Auto-rollback on critical metric degradation; escalate to on-call\n\n#### **Step 10: Feedback Loop & Learning**\n- **Input:** Developer feedback, deployment outcomes, incident reports\n- **Output:** Model retraining data, process improvement recommendations, updated rules\n- **Success Criteria:** Continuous improvement in review quality and deployment success rate\n- **Failure Handling:** Human review of feedback; gradual rule updates with A/B testing\n\n---\n\n### Question 1.2: Parallel vs. Blocking Steps & Critical Decision Points\n\n#### **Parallel Steps (Can Run Simultaneously):**\n1. **Step 2 (Static Analysis)** + **Step 3 (Security Scan)** + **Step 5 (Test Coverage)** → Independent analyses\n2. **Step 9 (Monitoring)** runs continuously in parallel with other operations\n\n#### **Blocking/Sequential Steps:**\n- **Step 1 (PR Intake)** → **BLOCKS** → Steps 2, 3, 4, 5 (need PR context)\n- **Steps 2, 3, 5** → **BLOCK** → **Step 4 (AI Review)** (review needs analysis results)\n- **Step 4 (AI Review)** → **BLOCKS** → **Step 7 (Deployment Decision)** (decision needs review approval)\n- **Step 6 (Build & Tests)** → **BLOCKS** → **Step 7** (can't deploy failed builds)\n- **Step 7 (Decision)** → **BLOCKS** → **Step 8 (Deployment)**\n- **Step 8 (Deployment)** → **BLOCKS** → **Step 9 (Monitoring)**\n\n#### **Critical Decision Points:**\n1. **After Step 1:** Is this PR high-risk? (determines review intensity)\n2. **After Step 3:** Are there CRITICAL security issues? (auto-reject if yes)\n3. **After Step 4:** Does AI review approve? (requires human if rejected)\n4. **After Step 6:** Did build/tests pass? (hard blocker)\n5. **After Step 7:** Deploy or wait for manual approval?\n6. **During Step 9:** Rollback or continue? (based on metrics)\n\n---\n\n### Question 1.3: Key Handoff Points & Data Context\n\n#### **Handoff 1: PR Intake → Analysis Steps (2, 3, 5)**\n- **Data Passed:** PR ID, file diffs, language/framework metadata, author info, target branch\n- **Context:** Repository structure, dependency manifest, previous PR history\n\n#### **Handoff 2: Analysis Steps → AI Code Review (Step 4)**\n- **Data Passed:** Aggregated analysis results (linting issues, security findings, coverage gaps)\n- **Context:** Coding standards, architectural guidelines, team preferences, historical review patterns\n\n#### **Handoff 3: AI Review → Build & Testing (Step 6)**\n- **Data Passed:** Review approval status, recommended changes, merged code preview\n- **Context:** CI/CD configuration, test suite, environment variables\n\n#### **Handoff 4: Build & AI Review → Deployment Decision (Step 7)**\n- **Data Passed:** Build status, test results, review summary, risk assessment\n- **Context:** Deployment policies, business priority, current production state, rollback capabilities\n\n#### **Handoff 5: Deployment Decision → Automated Deployment (Step 8)**\n- **Data Passed:** Deployment plan, target environment, feature flags, canary percentages\n- **Context:** Infrastructure state, traffic patterns, maintenance windows\n\n#### **Handoff 6: Deployment → Monitoring (Step 9)**\n- **Data Passed:** Deployment timestamp, version, change summary, baseline metrics\n- **Context:** Historical performance data, SLO/SLA thresholds, alert configurations\n\n#### **Handoff 7: Monitoring → Feedback Loop (Step 10)**\n- **Data Passed:** Deployment outcome, metric trends, incident reports, developer feedback\n- **Context:** Model performance metrics, false positive/negative rates, process bottlenecks"
  },
  {
   "cell_type": "markdown",
   "id": "cb38e9fa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fdc377",
   "metadata": {},
   "source": [
    "## Part B: AI Prompting Strategy (30 points)\n",
    "\n",
    "**Question 2.1:** For 2 consecutive major steps you identified, design specific AI prompts that would achieve the desired outcome. Include:\n",
    "- System role/persona definition\n",
    "- Structured input format\n",
    "- Expected output format\n",
    "- Examples of good vs bad responses\n",
    "- Error handling instructions\n",
    "\n",
    "**Question 2.2:** How would you handle the following challenging scenarios with your AI prompts:\n",
    "- **Code that uses obscure libraries or frameworks**\n",
    "- **Security reviews for code**\n",
    "- **Performance analysis of database queries**\n",
    "- **Legacy code modifications**\n",
    "\n",
    "**Question 2.3:** How would you ensure your prompts are working effectively and getting consistent results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd049f5",
   "metadata": {},
   "source": "## Response Part B:\n\n### Question 2.1: AI Prompts for Two Consecutive Steps\n\n#### **Prompt 1: Step 3 - Security Vulnerability Scan**\n\n```\nSYSTEM ROLE:\nYou are a senior security engineer specializing in application security and vulnerability assessment. Your expertise includes OWASP Top 10, secure coding practices, and common vulnerability patterns across multiple languages.\n\nINPUT FORMAT:\n{\n  \"pr_id\": \"string\",\n  \"code_diff\": \"unified diff format\",\n  \"dependencies_changed\": [\"package@version\"],\n  \"language\": \"string\",\n  \"framework\": \"string\"\n}\n\nTASK:\nAnalyze the code changes for security vulnerabilities. Focus on:\n1. Injection flaws (SQL, command, LDAP, etc.)\n2. Authentication/authorization bypasses\n3. Sensitive data exposure\n4. XML external entities (XXE)\n5. Security misconfigurations\n6. Cross-site scripting (XSS)\n7. Insecure deserialization\n8. Use of vulnerable dependencies\n9. Hardcoded secrets/credentials\n10. Insufficient logging/monitoring\n\nOUTPUT FORMAT (JSON):\n{\n  \"findings\": [\n    {\n      \"severity\": \"CRITICAL|HIGH|MEDIUM|LOW\",\n      \"category\": \"string (e.g., 'SQL Injection')\",\n      \"file\": \"path/to/file.py\",\n      \"line_number\": 42,\n      \"description\": \"Clear explanation of the vulnerability\",\n      \"evidence\": \"Code snippet showing the issue\",\n      \"remediation\": \"Specific fix recommendation with code example\",\n      \"cwe_id\": \"CWE-89\"\n    }\n  ],\n  \"summary\": {\n    \"critical_count\": 0,\n    \"high_count\": 0,\n    \"medium_count\": 0,\n    \"low_count\": 0,\n    \"overall_risk\": \"CRITICAL|HIGH|MEDIUM|LOW\"\n  },\n  \"dependency_alerts\": [...],\n  \"auto_reject\": true/false\n}\n\nGOOD RESPONSE EXAMPLE:\n{\n  \"findings\": [{\n    \"severity\": \"CRITICAL\",\n    \"category\": \"SQL Injection\",\n    \"file\": \"app/models/user.py\",\n    \"line_number\": 23,\n    \"description\": \"User input directly concatenated into SQL query without parameterization\",\n    \"evidence\": \"query = f'SELECT * FROM users WHERE id = {user_id}'\",\n    \"remediation\": \"Use parameterized queries: cursor.execute('SELECT * FROM users WHERE id = ?', (user_id,))\",\n    \"cwe_id\": \"CWE-89\"\n  }],\n  \"auto_reject\": true\n}\n\nBAD RESPONSE EXAMPLE:\n{\n  \"findings\": [{\n    \"severity\": \"HIGH\",\n    \"description\": \"Security issue found\",  // Too vague\n    \"file\": \"user.py\"  // Missing path and line number\n  }]\n  // Missing remediation, evidence, proper categorization\n}\n\nERROR HANDLING:\n- If code language/framework is unknown: Flag for manual security review, run generic SAST tools\n- If diff is too large (>10k lines): Request chunking or focus on high-risk areas (auth, data access)\n- If parsing fails: Return error with partial results and request human review\n- Always err on the side of caution: false positives are better than missed vulnerabilities\n```\n\n---\n\n#### **Prompt 2: Step 4 - AI Code Review (Consecutive Step)**\n\n```\nSYSTEM ROLE:\nYou are an expert code reviewer with 15+ years of experience across multiple languages and domains. You focus on code quality, maintainability, design patterns, and edge case handling. You provide constructive, actionable feedback.\n\nINPUT FORMAT:\n{\n  \"pr_context\": {\n    \"title\": \"string\",\n    \"description\": \"string\",\n    \"author\": \"string\",\n    \"files_changed\": [...],\n    \"code_diff\": \"unified diff\"\n  },\n  \"static_analysis_results\": {...},\n  \"security_scan_results\": {...},\n  \"coding_standards\": \"url or embedded doc\",\n  \"architectural_guidelines\": {...}\n}\n\nTASK:\nReview the code changes comprehensively. Your review should cover:\n1. **Readability**: Variable naming, function complexity, documentation\n2. **Maintainability**: DRY principle, modularity, testability\n3. **Design Patterns**: Appropriate use, anti-patterns to avoid\n4. **Edge Cases**: Null handling, error conditions, boundary values\n5. **Performance**: Algorithmic complexity, resource usage\n6. **Best Practices**: Language/framework-specific idioms\n7. **Integration with existing code**: Consistency, architectural fit\n\nOUTPUT FORMAT (JSON):\n{\n  \"overall_status\": \"APPROVED|CHANGES_REQUESTED|REJECTED\",\n  \"confidence_score\": 0.0-1.0,\n  \"review_comments\": [\n    {\n      \"file\": \"path/to/file\",\n      \"line\": 42,\n      \"severity\": \"BLOCKER|MAJOR|MINOR|SUGGESTION\",\n      \"category\": \"readability|maintainability|design|edge_case|performance|best_practice\",\n      \"comment\": \"Clear, actionable feedback\",\n      \"suggestion\": \"Specific code improvement (optional)\",\n      \"rationale\": \"Why this matters\"\n    }\n  ],\n  \"summary\": {\n    \"strengths\": [\"What was done well\"],\n    \"concerns\": [\"Key issues to address\"],\n    \"estimated_fix_time\": \"2 hours\"\n  },\n  \"requires_human_review\": true/false,\n  \"human_review_reason\": \"optional explanation\"\n}\n\nGOOD RESPONSE EXAMPLE:\n{\n  \"overall_status\": \"CHANGES_REQUESTED\",\n  \"confidence_score\": 0.85,\n  \"review_comments\": [{\n    \"file\": \"app/services/payment.py\",\n    \"line\": 67,\n    \"severity\": \"BLOCKER\",\n    \"category\": \"edge_case\",\n    \"comment\": \"Payment amount not validated for negative values. This could allow refund fraud.\",\n    \"suggestion\": \"if amount <= 0: raise ValueError('Amount must be positive')\",\n    \"rationale\": \"Financial transactions must validate input to prevent fraud and data corruption\"\n  }],\n  \"summary\": {\n    \"strengths\": [\"Clean error handling\", \"Well-documented functions\"],\n    \"concerns\": [\"Missing input validation\", \"No tests for edge cases\"],\n    \"estimated_fix_time\": \"2 hours\"\n  }\n}\n\nBAD RESPONSE EXAMPLE:\n{\n  \"status\": \"bad\",  // Wrong field name\n  \"comments\": \"Code looks fine\"  // No structure, no actionable feedback\n}\n\nERROR HANDLING:\n- If context is insufficient: Request additional information (architectural docs, related code)\n- If confidence score <0.6: Flag for mandatory human review\n- If conflicting with static analysis: Defer to static analysis for objective issues\n- If unclear coding standards: Use industry best practices, note assumption\n```\n\n---\n\n### Question 2.2: Handling Challenging Scenarios\n\n#### **1. Code Using Obscure Libraries/Frameworks**\n\n**Strategy:**\n```\nEnhanced Prompt Addition:\n\n\"For unfamiliar libraries/frameworks:\n1. Search internal knowledge base for library documentation\n2. If not found, analyze import statements and usage patterns\n3. Focus review on:\n   - General coding principles (still applicable)\n   - Error handling around library calls\n   - Resource management (connections, files, memory)\n   - Security implications (data flow, external calls)\n4. Flag specific library usage patterns for human expert review\n5. Include in review: 'Library [name] is unfamiliar - recommend expert review for [specific concerns]'\n6. Don't make assumptions about library behavior - be explicit about uncertainty\"\n\nExample Comment:\n\"This code uses 'obscure-lib' which is not in my knowledge base. The general pattern looks sound, but I recommend expert review for:\n- Proper initialization sequence (line 45)\n- Thread safety of client instance (line 67)\n- Resource cleanup in edge cases (line 89)\"\n```\n\n#### **2. Security Reviews for Code**\n\n**Strategy:**\n```\nMulti-Layer Approach:\n\nLayer 1: Pattern Matching\n- Use regex + AST parsing for known vulnerability patterns\n- Check against CWE/SANS Top 25 dangerous functions\n- Scan for hardcoded secrets, weak crypto, injection points\n\nLayer 2: Data Flow Analysis\n- Track user input from entry points to sensitive operations\n- Identify sanitization/validation gaps\n- Map authentication/authorization checks\n\nLayer 3: Contextual AI Review (Enhanced Prompt):\n\"Security-Focused Directives:\n- Assume all external input is malicious\n- Verify authentication before authorization\n- Check for race conditions in security-critical code\n- Validate crypto is FIPS 140-2 compliant\n- Ensure sensitive data is encrypted at rest and in transit\n- Look for timing attacks in authentication\n- Verify no sensitive data in logs/error messages\n- If uncertain about security implications: escalate to security team\"\n\nConfidence Threshold:\n- Auto-reject if CRITICAL severity + high confidence (>0.8)\n- Require security team review if CRITICAL + low confidence (<0.6)\n```\n\n#### **3. Performance Analysis of Database Queries**\n\n**Strategy:**\n```\nSpecialized Sub-Agent Prompt:\n\nSYSTEM ROLE:\nYou are a database performance specialist with expertise in query optimization, indexing strategies, and ORM usage patterns.\n\nANALYSIS CHECKLIST:\n1. **N+1 Query Detection**\n   - Look for loops containing database queries\n   - Flag ORM lazy-loading in iteration contexts\n   - Suggest eager loading or join strategies\n\n2. **Index Usage**\n   - Check WHERE/JOIN clauses align with existing indexes\n   - Identify full table scans on large tables\n   - Recommend composite indexes for multi-column filters\n\n3. **Query Complexity**\n   - Count JOINs (>5 is concerning)\n   - Check for SELECT * (select only needed columns)\n   - Identify missing LIMIT clauses on potentially large result sets\n\n4. **ORM Anti-Patterns**\n   - Avoid business logic in queries\n   - Check for proper transaction boundaries\n   - Verify connection pooling usage\n\nOUTPUT INCLUDES:\n- EXPLAIN plan analysis (if available)\n- Estimated rows scanned vs. returned\n- Recommended indexes with CREATE INDEX statements\n- Rewritten query examples\n- Performance impact estimate (e.g., \"Expected 100x speedup\")\n```\n\n#### **4. Legacy Code Modifications**\n\n**Strategy:**\n```\nContext-Aware Legacy Code Prompt:\n\n\"LEGACY CODE REVIEW MODE:\n\nYou are reviewing changes to legacy code (written >3 years ago, potentially using outdated patterns).\n\nSPECIAL CONSIDERATIONS:\n1. **Backwards Compatibility**\n   - Flag any breaking API changes\n   - Verify existing callers aren't affected\n   - Check for database migration requirements\n\n2. **Technical Debt Assessment**\n   - Document existing anti-patterns being perpetuated\n   - Suggest incremental improvements (don't demand full refactor)\n   - Balance ideal vs. pragmatic\n\n3. **Risk Minimization**\n   - Prefer minimal changes over extensive refactoring\n   - Require comprehensive tests for any behavior changes\n   - Recommend feature flags for risky changes\n\n4. **Documentation Requirements**\n   - Higher bar for comments (future maintainers need context)\n   - Document 'why' not just 'what'\n   - Explain workarounds for legacy constraints\n\nREVIEW TONE:\n- Acknowledge constraints of legacy systems\n- Suggest improvements as 'nice-to-have' vs. 'must-fix'\n- Provide migration path for technical debt\n\nEXAMPLE COMMENT:\n'This perpetuates the existing pattern of [anti-pattern]. While ideally we'd refactor to [better approach], given the legacy constraints, this change is acceptable. Consider filing a tech debt ticket for future refactoring.'\"\n```\n\n---\n\n### Question 2.3: Ensuring Prompt Effectiveness & Consistency\n\n#### **1. Continuous Validation Framework**\n\n**A. Automated Testing**\n```\n- Maintain test suite of 100+ PRs with known issues\n- Run prompts against test suite weekly\n- Track metrics:\n  * True positive rate (found real issues)\n  * False positive rate (flagged non-issues)\n  * False negative rate (missed real issues)\n  * Consistency score (same input → same output)\n- Set thresholds: TP >80%, FP <15%, FN <10%\n```\n\n**B. A/B Testing**\n```\n- Run 10% of PRs through old vs. new prompt versions\n- Compare developer feedback ratings\n- Measure time-to-approval\n- Gradual rollout if new version performs better\n```\n\n#### **2. Human Feedback Loop**\n\n**Developer Feedback Collection:**\n```\nAfter each AI review:\n- \"Was this feedback helpful?\" (1-5 rating)\n- \"Which comments were most/least valuable?\" (multi-select)\n- \"What did we miss?\" (free text)\n- \"False positives?\" (flagging system)\n\nWeekly aggregation:\n- Low-rated reviews → prompt refinement\n- Frequently dismissed comments → adjust severity or remove\n- Missed issues → add to training examples\n```\n\n#### **3. Golden Dataset Curation**\n\n```\nMaintain \"Golden PR Set\":\n- 50 PRs with expert-validated reviews\n- Covers diverse scenarios (languages, complexity, domains)\n- Update quarterly with new patterns\n- Use for prompt regression testing\n\nBefore deploying prompt changes:\n- Run against golden dataset\n- Require 95% agreement with expert reviews\n- Manual review of disagreements\n```\n\n#### **4. Prompt Versioning & Rollback**\n\n```\nVersion Control for Prompts:\n- Git-tracked prompt templates\n- Semantic versioning (major.minor.patch)\n- Change logs with rationale\n- Ability to rollback to previous version within 5 minutes\n\nDeployment Strategy:\n- Canary release: 1% → 5% → 25% → 100%\n- Monitor error rates at each stage\n- Auto-rollback if error rate >2x baseline\n```\n\n#### **5. Output Consistency Checks**\n\n**Automated Validation:**\n```python\ndef validate_ai_output(output):\n    \"\"\"Ensure AI response matches expected schema\"\"\"\n    required_fields = ['overall_status', 'confidence_score', 'review_comments']\n    \n    # Schema validation\n    assert all(field in output for field in required_fields)\n    \n    # Business logic validation\n    assert 0 <= output['confidence_score'] <= 1\n    assert output['overall_status'] in ['APPROVED', 'CHANGES_REQUESTED', 'REJECTED']\n    \n    # Consistency checks\n    if output['overall_status'] == 'REJECTED':\n        assert any(c['severity'] == 'BLOCKER' for c in output['review_comments'])\n    \n    # Flag for human review if confidence low\n    if output['confidence_score'] < 0.6:\n        output['requires_human_review'] = True\n    \n    return output\n```\n\n#### **6. Calibration with Human Reviewers**\n\n```\nMonthly Calibration Sessions:\n- Select 20 random PRs reviewed by AI\n- Panel of 3 senior engineers independently review\n- Compare AI feedback vs. human consensus\n- Identify systematic biases\n- Update prompts to align with human judgment\n\nMetrics to track:\n- Severity alignment (is AI too strict/lenient?)\n- Category accuracy (correct issue classification?)\n- Actionability score (can devs act on feedback?)\n```"
  },
  {
   "cell_type": "markdown",
   "id": "476e98d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59d353d",
   "metadata": {},
   "source": [
    "## Part C: System Architecture & Reusability (25 points)\n",
    "\n",
    "**Question 3.1:** How would you make this system reusable across different projects/teams? Consider:\n",
    "- Configuration management\n",
    "- Language/framework variations\n",
    "- Different deployment targets (cloud providers, on-prem)\n",
    "- Team-specific coding standards\n",
    "- Industry-specific compliance requirements\n",
    "\n",
    "**Question 3.2:** How would the system get better over time based on:\n",
    "- False positive/negative rates in reviews\n",
    "- Deployment success/failure patterns\n",
    "- Developer feedback\n",
    "- Production incident correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0052f045",
   "metadata": {},
   "source": "## Response Part C:\n\n### Question 3.1: System Reusability Across Projects/Teams\n\n#### **1. Configuration Management**\n\n**Multi-Tier Configuration System:**\n\n```yaml\n# Tier 1: Global Defaults (system-wide)\nglobal_config.yaml:\n  review_thresholds:\n    security_critical_auto_reject: true\n    min_test_coverage: 80\n    max_pr_size_lines: 500\n  deployment:\n    default_strategy: \"blue-green\"\n    rollback_threshold_error_rate: 5.0\n    monitoring_duration_minutes: 15\n\n# Tier 2: Organization Level (company-wide)\norg_config.yaml:\n  security_compliance:\n    required_scans: [\"SAST\", \"dependency\", \"secrets\"]\n    pci_dss_enabled: true\n  deployment_targets:\n    - name: \"aws-prod\"\n      region: \"us-east-1\"\n    - name: \"azure-staging\"\n      region: \"eastus\"\n\n# Tier 3: Team Level (overrides org)\nteam_backend_config.yaml:\n  coding_standards:\n    url: \"https://wiki.company.com/backend-standards\"\n    linters: [\"pylint\", \"mypy\", \"black\"]\n  review_rules:\n    min_approvers: 2\n    require_architecture_review_if_lines_changed: 1000\n\n# Tier 4: Project Level (most specific)\nproject_api_config.yaml:\n  language: \"python\"\n  framework: \"fastapi\"\n  custom_checks:\n    - \"check_api_versioning\"\n    - \"validate_openapi_spec\"\n  deployment:\n    strategy: \"canary\"  # Override org default\n    canary_percentages: [10, 25, 50, 100]\n```\n\n**Configuration Inheritance:**\n```\nProject → Team → Org → Global\n(Most specific wins)\n```\n\n---\n\n#### **2. Language/Framework Variations**\n\n**Plugin Architecture:**\n\n```python\n# Base Plugin Interface\nclass ReviewPlugin(ABC):\n    @abstractmethod\n    def analyze(self, code_context: CodeContext) -> List[Issue]:\n        pass\n    \n    @abstractmethod\n    def supports(self, language: str, framework: str) -> bool:\n        pass\n\n# Language-Specific Plugins\nclass PythonReviewPlugin(ReviewPlugin):\n    def analyze(self, context):\n        # Python-specific checks\n        issues = []\n        issues.extend(check_type_hints(context))\n        issues.extend(check_pep8(context))\n        return issues\n    \n    def supports(self, lang, framework):\n        return lang == \"python\"\n\nclass JavaReviewPlugin(ReviewPlugin):\n    def analyze(self, context):\n        # Java-specific checks\n        issues = []\n        issues.extend(check_null_safety(context))\n        issues.extend(check_spring_patterns(context))\n        return issues\n\n# Plugin Registry\nPLUGIN_REGISTRY = {\n    \"python\": [PythonReviewPlugin(), DjangoPlugin(), FastAPIPlugin()],\n    \"java\": [JavaReviewPlugin(), SpringBootPlugin()],\n    \"javascript\": [JavaScriptReviewPlugin(), ReactPlugin(), NodePlugin()],\n    # ...\n}\n\n# Auto-discovery at runtime\ndef get_plugins(language, framework):\n    base_plugins = PLUGIN_REGISTRY.get(language, [])\n    return [p for p in base_plugins if p.supports(language, framework)]\n```\n\n---\n\n#### **3. Different Deployment Targets**\n\n**Abstract Deployment Interface:**\n\n```python\nclass DeploymentTarget(ABC):\n    @abstractmethod\n    def deploy(self, artifact, environment, strategy):\n        pass\n    \n    @abstractmethod\n    def rollback(self, deployment_id):\n        pass\n    \n    @abstractmethod\n    def health_check(self, deployment_id):\n        pass\n\n# Cloud Provider Implementations\nclass AWSDeployment(DeploymentTarget):\n    def deploy(self, artifact, environment, strategy):\n        # Use AWS ECS/EKS/Lambda based on artifact type\n        if strategy == \"blue-green\":\n            return self._deploy_blue_green_ecs(artifact, environment)\n        elif strategy == \"canary\":\n            return self._deploy_canary_appmesh(artifact, environment)\n\nclass AzureDeployment(DeploymentTarget):\n    def deploy(self, artifact, environment, strategy):\n        # Use Azure App Service/AKS/Functions\n        if strategy == \"blue-green\":\n            return self._deploy_blue_green_slots(artifact, environment)\n\nclass OnPremKubernetesDeployment(DeploymentTarget):\n    def deploy(self, artifact, environment, strategy):\n        # Use Helm/Flux/ArgoCD\n        if strategy == \"canary\":\n            return self._deploy_canary_istio(artifact, environment)\n\n# Factory Pattern\ndef get_deployment_target(target_config):\n    provider = target_config['provider']\n    if provider == 'aws':\n        return AWSDeployment(target_config)\n    elif provider == 'azure':\n        return AzureDeployment(target_config)\n    elif provider == 'kubernetes':\n        return OnPremKubernetesDeployment(target_config)\n```\n\n---\n\n#### **4. Team-Specific Coding Standards**\n\n**Dynamic Standards Loading:**\n\n```python\nclass CodingStandardsManager:\n    def __init__(self):\n        self.cache = {}\n    \n    def load_standards(self, team_id):\n        \"\"\"Load team-specific standards with fallbacks\"\"\"\n        if team_id in self.cache:\n            return self.cache[team_id]\n        \n        standards = {\n            'global': self._load_from_url('https://standards.company.com/global'),\n            'team': self._load_from_url(f'https://standards.company.com/teams/{team_id}'),\n            'custom_rules': self._load_custom_rules(team_id)\n        }\n        \n        # Merge with priority: custom > team > global\n        merged = self._merge_standards(standards)\n        self.cache[team_id] = merged\n        return merged\n    \n    def validate_code(self, code, team_id):\n        standards = self.load_standards(team_id)\n        violations = []\n        \n        for rule in standards['rules']:\n            if rule.check(code):\n                violations.append(rule.create_violation())\n        \n        return violations\n```\n\n---\n\n#### **5. Industry-Specific Compliance Requirements**\n\n**Compliance Plugin System:**\n\n```python\nclass CompliancePlugin(ABC):\n    @abstractmethod\n    def validate(self, code_change, deployment_plan):\n        pass\n    \n    @abstractmethod\n    def get_required_approvals(self):\n        pass\n\nclass HIPAACompliancePlugin(CompliancePlugin):\n    \"\"\"Healthcare data compliance\"\"\"\n    def validate(self, code_change, deployment_plan):\n        checks = []\n        # PHI data handling\n        checks.append(self._check_phi_encryption(code_change))\n        # Audit logging\n        checks.append(self._check_audit_trail(code_change))\n        # Access controls\n        checks.append(self._check_authorization(code_change))\n        return checks\n    \n    def get_required_approvals(self):\n        return ['security-officer', 'compliance-lead']\n\nclass PCI_DSS_CompliancePlugin(CompliancePlugin):\n    \"\"\"Payment card industry compliance\"\"\"\n    def validate(self, code_change, deployment_plan):\n        checks = []\n        # Cardholder data\n        checks.append(self._check_card_data_storage(code_change))\n        # Network segmentation\n        checks.append(self._check_network_isolation(deployment_plan))\n        # Encryption standards\n        checks.append(self._check_encryption_standards(code_change))\n        return checks\n\nclass SOC2CompliancePlugin(CompliancePlugin):\n    \"\"\"SOC 2 Type II compliance\"\"\"\n    def validate(self, code_change, deployment_plan):\n        checks = []\n        # Change management\n        checks.append(self._check_change_approval_trail(code_change))\n        # Data retention\n        checks.append(self._check_data_retention_policy(code_change))\n        return checks\n\n# Compliance Registry\nCOMPLIANCE_PLUGINS = {\n    'healthcare': HIPAACompliancePlugin(),\n    'finance': PCI_DSS_CompliancePlugin(),\n    'saas': SOC2CompliancePlugin()\n}\n\n# Configuration\nproject_config = {\n    'industry': 'finance',\n    'compliance_requirements': ['PCI-DSS', 'SOC2']\n}\n\n# At runtime\nactive_compliance = [COMPLIANCE_PLUGINS[req.lower()] for req in project_config['compliance_requirements']]\n```\n\n---\n\n### Question 3.2: Continuous Improvement Mechanisms\n\n#### **1. False Positive/Negative Rate Tracking**\n\n**Feedback Collection System:**\n\n```python\nclass FeedbackTracker:\n    def record_developer_feedback(self, pr_id, review_comment_id, feedback):\n        \"\"\"Track which AI comments were helpful/unhelpful\"\"\"\n        self.db.insert({\n            'pr_id': pr_id,\n            'comment_id': review_comment_id,\n            'feedback_type': feedback['type'],  # 'false_positive', 'helpful', 'unhelpful'\n            'category': feedback['category'],\n            'developer_note': feedback['note'],\n            'timestamp': datetime.now()\n        })\n    \n    def analyze_false_positive_patterns(self):\n        \"\"\"Identify systematic issues\"\"\"\n        query = \"\"\"\n            SELECT category, COUNT(*) as count, \n                   AVG(CASE WHEN feedback_type = 'false_positive' THEN 1 ELSE 0 END) as fp_rate\n            FROM feedback\n            GROUP BY category\n            HAVING fp_rate > 0.15  -- 15% threshold\n            ORDER BY fp_rate DESC\n        \"\"\"\n        return self.db.execute(query)\n    \n    def retrain_model(self):\n        \"\"\"Use feedback to improve prompts\"\"\"\n        patterns = self.analyze_false_positive_patterns()\n        \n        for pattern in patterns:\n            if pattern['category'] == 'naming_convention':\n                # Adjust prompt to be less strict on naming\n                self.update_prompt_template(\n                    category='naming',\n                    adjustment='lower_severity_for_minor_violations'\n                )\n\n**Automated Retraining Pipeline:**\n```\nWeekly:\n1. Aggregate developer feedback\n2. Identify top 5 false positive categories\n3. Generate prompt adjustment recommendations\n4. A/B test new prompts on 10% of PRs\n5. If improvement > 5%, gradual rollout\n\nMonthly:\n1. Retrain ML models (if using fine-tuned models)\n2. Update rule engines based on miss patterns\n3. Refresh knowledge base with new library patterns\n```\n\n---\n\n#### **2. Deployment Success/Failure Pattern Analysis**\n\n**Predictive Failure Detection:**\n\n```python\nclass DeploymentLearner:\n    def record_deployment_outcome(self, deployment_id, outcome):\n        \"\"\"Track what led to successful/failed deployments\"\"\"\n        deployment = self.get_deployment_metadata(deployment_id)\n        \n        features = {\n            'code_complexity': deployment['complexity_score'],\n            'test_coverage': deployment['coverage_percentage'],\n            'security_issues_fixed': deployment['security_findings'],\n            'review_time_hours': deployment['review_duration'],\n            'lines_changed': deployment['diff_size'],\n            'num_files_changed': deployment['file_count'],\n            'has_db_migration': deployment['has_migration'],\n            'deployment_hour': deployment['deployed_at'].hour,\n            'deployment_day_of_week': deployment['deployed_at'].weekday(),\n        }\n        \n        self.training_data.append({\n            'features': features,\n            'outcome': outcome,  # 'success', 'rolled_back', 'partial_failure'\n            'failure_reason': deployment.get('failure_reason')\n        })\n    \n    def predict_deployment_risk(self, deployment_plan):\n        \"\"\"Use historical data to predict risk\"\"\"\n        features = self.extract_features(deployment_plan)\n        risk_score = self.model.predict_proba(features)[0][1]  # Probability of failure\n        \n        if risk_score > 0.3:\n            return {\n                'risk_level': 'HIGH',\n                'recommendation': 'Deploy to staging first, extended monitoring',\n                'similar_failures': self.find_similar_failed_deployments(features)\n            }\n        elif risk_score > 0.15:\n            return {\n                'risk_level': 'MEDIUM',\n                'recommendation': 'Use canary deployment, watch metrics closely'\n            }\n        else:\n            return {'risk_level': 'LOW', 'recommendation': 'Standard deployment OK'}\n```\n\n---\n\n#### **3. Developer Feedback Integration**\n\n**Sentiment Analysis & Action Items:**\n\n```python\nclass DeveloperFeedbackAnalyzer:\n    def analyze_review_quality(self, pr_id):\n        \"\"\"Collect and analyze developer satisfaction\"\"\"\n        feedback = self.get_developer_survey(pr_id)\n        \n        sentiment = self.nlp_model.analyze_sentiment(feedback['comments'])\n        \n        if sentiment['score'] < 3.0:  # Out of 5\n            # Extract what was wrong\n            pain_points = self.extract_pain_points(feedback['comments'])\n            \n            for pain_point in pain_points:\n                self.create_improvement_ticket({\n                    'category': pain_point['category'],\n                    'description': pain_point['description'],\n                    'frequency': pain_point['occurrence_count'],\n                    'priority': self.calculate_priority(pain_point)\n                })\n    \n    def identify_trending_complaints(self):\n        \"\"\"Find common themes in negative feedback\"\"\"\n        recent_feedback = self.db.query(\"\"\"\n            SELECT feedback_text, rating\n            FROM developer_feedback\n            WHERE created_at > NOW() - INTERVAL '30 days'\n              AND rating < 3\n        \"\"\")\n        \n        # Topic modeling\n        topics = self.topic_model.fit_transform([f['feedback_text'] for f in recent_feedback])\n        \n        # Examples:\n        # - Topic 1: \"Too many style nitpicks\" → Adjust linter strictness\n        # - Topic 2: \"Missed critical bug\" → Improve edge case detection\n        # - Topic 3: \"Slow review time\" → Optimize pipeline\n        \n        return topics\n```\n\n---\n\n#### **4. Production Incident Correlation**\n\n**Incident-to-PR Mapping:**\n\n```python\nclass IncidentCorrelation:\n    def link_incident_to_deployment(self, incident_id):\n        \"\"\"Find which code change caused the incident\"\"\"\n        incident = self.get_incident(incident_id)\n        \n        # Find deployments around incident time\n        recent_deployments = self.db.query(\"\"\"\n            SELECT * FROM deployments\n            WHERE deployed_at BETWEEN %s AND %s\n              AND environment = 'production'\n            ORDER BY deployed_at DESC\n        \"\"\", (incident['start_time'] - timedelta(hours=24), incident['start_time']))\n        \n        # Analyze which PR likely caused it\n        for deployment in recent_deployments:\n            pr = self.get_pr(deployment['pr_id'])\n            \n            # Check if PR modified affected service/file\n            if self.pr_affects_service(pr, incident['affected_service']):\n                confidence = self.calculate_causation_confidence(pr, incident)\n                \n                if confidence > 0.7:\n                    # Record for learning\n                    self.record_missed_issue({\n                        'pr_id': pr['id'],\n                        'incident_type': incident['type'],\n                        'root_cause': incident['root_cause'],\n                        'what_review_missed': self.analyze_review_gap(pr, incident)\n                    })\n    \n    def improve_prompts_from_incidents(self):\n        \"\"\"Update AI prompts to catch similar issues\"\"\"\n        missed_issues = self.db.query(\"\"\"\n            SELECT what_review_missed, COUNT(*) as frequency\n            FROM incident_root_causes\n            GROUP BY what_review_missed\n            ORDER BY frequency DESC\n            LIMIT 10\n        \"\"\")\n        \n        for issue in missed_issues:\n            if issue['what_review_missed'] == 'race_condition':\n                self.add_to_prompt(\n                    section='concurrency_checks',\n                    new_rule='Check for unsynchronized access to shared state'\n                )\n            elif issue['what_review_missed'] == 'null_pointer':\n                self.add_to_prompt(\n                    section='null_safety',\n                    new_rule='Require null checks before dereferencing, especially for external API responses'\n                )\n```\n\n**Continuous Improvement Metrics Dashboard:**\n```\nWeekly KPIs:\n- False Positive Rate: 12% → Target <10%\n- False Negative Rate (from incidents): 8% → Target <5%\n- Developer Satisfaction: 4.2/5 → Target >4.5/5\n- Deployment Success Rate: 94% → Target >98%\n- Mean Time to Review: 2.3 hours → Target <2 hours\n\nImprovement Actions Queue:\n1. [HIGH] Reduce false positives for \"variable naming\" (18% FP rate)\n2. [HIGH] Add checks for database transaction boundaries (caused 3 incidents)\n3. [MEDIUM] Speed up security scans (currently 45s, target 30s)\n4. [LOW] Support Rust language (3 teams requesting)\n```"
  },
  {
   "cell_type": "markdown",
   "id": "6029f169",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d096eb",
   "metadata": {},
   "source": [
    "## Part D: Implementation Strategy (20 points)\n",
    "\n",
    "**Question 4.1:** Prioritize your implementation. What would you build first? Create a 6-month roadmap with:\n",
    "- MVP definition (what's the minimum viable system?)\n",
    "- Pilot program strategy\n",
    "- Rollout phases\n",
    "- Success metrics for each phase\n",
    "\n",
    "**Question 4.2:** Risk mitigation. What could go wrong and how would you handle:\n",
    "- AI making incorrect review decisions\n",
    "- System downtime during critical deployments\n",
    "- Integration failures with existing tools\n",
    "- Resistance from development teams\n",
    "- Compliance/audit requirements\n",
    "\n",
    "**Question 4.3:** Tool selection. What existing tools/platforms would you integrate with or build upon:\n",
    "- Code review platforms (GitHub, GitLab, Bitbucket)\n",
    "- CI/CD systems (Jenkins, GitHub Actions, GitLab CI)\n",
    "- Monitoring tools (Datadog, New Relic, Prometheus)\n",
    "- Security scanning tools (SonarQube, Snyk, Veracode)\n",
    "- Communication tools (Slack, Teams, Jira)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fa9820",
   "metadata": {},
   "source": "## Response Part D:\n\n### Question 4.1: 6-Month Implementation Roadmap\n\n#### **Month 1-2: MVP (Minimum Viable Product)**\n\n**Goal:** Prove core value with minimal scope\n\n**MVP Features:**\n1. **Basic AI Code Review** (Single language: Python)\n   - Static analysis integration (pylint, mypy)\n   - Security scan (basic SAST + secrets detection)\n   - AI review for readability & common bugs\n   - Output: Structured comments on PR\n\n2. **Simple Deployment Pipeline** (Single environment: Staging)\n   - GitHub Actions integration\n   - Build + run tests\n   - Deploy to staging (blue-green only)\n   - Basic health checks\n\n3. **Manual Override** (Always available)\n   - Developers can skip AI review with justification\n   - All deployments require human approval\n\n**Success Metrics:**\n- 80% of PRs receive AI feedback within 10 minutes\n- AI catches 50%+ of issues found in human review\n- Zero false deployments to production\n- Developer satisfaction >3.5/5\n\n**Pilot Program:**\n- **Target:** 1-2 small teams (10-15 developers)\n- **Projects:** Non-critical internal tools\n- **Duration:** 4 weeks\n- **Weekly feedback sessions**\n\n---\n\n#### **Month 3: Expansion & Refinement**\n\n**New Features:**\n1. **Multi-language Support**\n   - Add JavaScript/TypeScript\n   - Java support\n\n2. **Enhanced Security**\n   - Dependency vulnerability scanning\n   - OWASP Top 10 checks\n   - Integration with Snyk/Veracode\n\n3. **Deployment Automation**\n   - Auto-deploy to production (low-risk PRs only)\n   - Canary deployment support\n   - Rollback automation\n\n**Pilot Expansion:**\n- Add 2-3 more teams\n- Include 1 production service (low-traffic)\n- A/B test: AI review vs. traditional review speed\n\n**Success Metrics:**\n- Review time <4 hours for 70% of PRs\n- Security vulnerability detection >85%\n- Deployment success rate >95%\n- Developer satisfaction >4.0/5\n\n---\n\n#### **Month 4: Production Readiness**\n\n**New Features:**\n1. **Advanced Deployment Strategies**\n   - Feature flags integration\n   - Multi-region deployments\n   - Database migration handling\n\n2. **Observability**\n   - Post-deployment monitoring\n   - Anomaly detection\n   - Auto-rollback on metric degradation\n\n3. **Compliance & Audit**\n   - Audit trail for all reviews & deployments\n   - Compliance checks (SOC2, PCI-DSS)\n   - Approval workflows for sensitive changes\n\n**Rollout:**\n- Expand to 50% of development teams\n- Include 5-10 production services\n- Mandatory for new microservices\n\n**Success Metrics:**\n- Review time <4 hours for 85% of PRs\n- Security detection >90%\n- Zero critical prod incidents from missed reviews\n- Deployment frequency increased by 2x\n\n---\n\n#### **Month 5-6: Full Rollout & Optimization**\n\n**Final Features:**\n1. **Full Language Support**\n   - Go, Rust, PHP, Ruby\n   - Framework-specific checks\n\n2. **Advanced AI Capabilities**\n   - Performance optimization suggestions\n   - Architecture pattern recommendations\n   - Test generation suggestions\n\n3. **Self-Service Configuration**\n   - Team-specific rule customization\n   - Custom deployment workflows\n   - Integration marketplace (Jira, Slack, etc.)\n\n**Rollout:**\n- 100% of teams onboarded\n- All production deployments go through system\n- Human review required only for flagged PRs\n\n**Success Metrics (Final):**\n- Review time <4 hours for 90% of PRs\n- Security vulnerability detection >90%\n- Deployment success rate >98%\n- Developer satisfaction >4.5/5\n- Deployment frequency increased by 3x\n- Rollback rate <2%\n\n---\n\n### Question 4.2: Risk Mitigation Strategies\n\n#### **1. AI Making Incorrect Review Decisions**\n\n**Risk:** AI approves bad code or rejects good code\n\n**Mitigation:**\n```\nPREVENTION:\n- Confidence scoring: Require human review if confidence <0.7\n- Golden dataset testing before deploying prompt changes\n- Gradual rollout of new AI models (1% → 5% → 25% → 100%)\n- Multiple validation layers (static analysis + AI + security scan)\n\nDETECTION:\n- Track false positive/negative rates from developer feedback\n- Monitor incident correlation (did AI-reviewed PR cause production issue?)\n- Weekly review of AI-rejected PRs that developers overrode\n\nRESPONSE:\n- Immediate rollback capability for prompt versions\n- Human escalation workflow for disputed reviews\n- Post-mortem for every missed critical issue\n- Continuous prompt refinement based on feedback\n\nSAFEGUARDS:\n- Never auto-deploy to production without tests passing\n- Always allow manual override with justification\n- Critical services require human review (even if AI approves)\n- Security issues always escalate to security team\n```\n\n---\n\n#### **2. System Downtime During Critical Deployments**\n\n**Risk:** Pipeline fails during time-sensitive deployment (hotfix, security patch)\n\n**Mitigation:**\n```\nPREVENTION:\n- 99.9% SLA for core pipeline services\n- Multi-region redundancy (active-active)\n- Circuit breaker pattern (fallback to manual process)\n- Regular disaster recovery drills\n\nDETECTION:\n- Real-time health monitoring with <30s alert latency\n- Canary deployments for pipeline changes\n- Synthetic transactions running every 60s\n\nRESPONSE:\n- Auto-failover to backup region (<2 min)\n- Emergency bypass mode (skip AI review, auto-approve)\n  * Requires VP Engineering approval\n  * Mandatory post-deployment review\n  * Auto-creates follow-up ticket\n- Incident response playbook with clear roles\n\nCOMMUNICATION:\n- Status page for pipeline health\n- Automatic Slack alerts for degraded service\n- SMS escalation for critical failures\n```\n\n---\n\n#### **3. Integration Failures with Existing Tools**\n\n**Risk:** GitHub/GitLab API changes break our integration, data sync issues\n\n**Mitigation:**\n```\nPREVENTION:\n- Abstract integration layer (adapter pattern)\n- Version pinning with gradual upgrade strategy\n- Integration tests running against prod-like environments\n- Sandbox environments for testing integrations\n\nDETECTION:\n- Monitor API response codes and latency\n- Alert on increased error rates (>1%)\n- Daily integration health checks\n- Quarterly dependency audits\n\nRESPONSE:\n- Rollback to previous integration version\n- Graceful degradation (e.g., skip optional features)\n- Rate limiting and retry logic with exponential backoff\n- Vendor escalation contacts for critical issues\n\nREDUNDANCY:\n- Support multiple code hosting platforms (GitHub + GitLab)\n- Multi-cloud deployment (don't depend on single CI/CD platform)\n```\n\n---\n\n#### **4. Resistance from Development Teams**\n\n**Risk:** Developers bypass system, ignore feedback, or refuse adoption\n\n**Mitigation:**\n```\nSTAKEHOLDER ENGAGEMENT:\n- Involve senior engineers in design phase\n- Monthly feedback sessions with developers\n- Champions program (early adopters advocate for system)\n- Transparent roadmap with user-requested features\n\nTRUST BUILDING:\n- Start with \"advisory mode\" (suggestions, not blockers)\n- Show value with metrics (time saved, bugs caught)\n- Quick wins: automate annoying tasks (changelog generation, release notes)\n- Acknowledge when AI is wrong, fix quickly\n\nINCENTIVES:\n- Gamification: Recognition for teams with best code quality improvement\n- Reduced manual review burden (senior engineers freed up)\n- Faster deployments = faster feature delivery\n- Include \"AI review quality\" in engineering metrics (but don't penalize)\n\nADDRESSING CONCERNS:\n- \"AI will replace me\": Position as \"AI augments, doesn't replace\"\n- \"AI is too strict\": Allow team-specific configuration\n- \"AI doesn't understand context\": Improve prompts, add context input fields\n- \"Takes too long\": Optimize for speed, show time savings metrics\n\nMANDATORY ADOPTION (Phased):\n- Month 1-2: Optional\n- Month 3-4: Required for non-critical services\n- Month 5-6: Required for all services\n- Exceptions process for edge cases\n```\n\n---\n\n#### **5. Compliance/Audit Requirements**\n\n**Risk:** Fail regulatory audit, can't prove code review happened, liability issues\n\n**Mitigation:**\n```\nAUDIT TRAIL:\n- Immutable log of all reviews & deployments (append-only, signed)\n- Retention: 7 years (compliance with SOX, GDPR)\n- Include: Who, What, When, Why for every change\n- Evidence: code diff, review comments, approval chain, deployment logs\n\nDATA INTEGRITY:\n- Cryptographic signing of review results\n- Tamper-evident logs (blockchain or Merkle trees)\n- Regular integrity audits (quarterly)\n- Backup to separate compliance storage (cold storage)\n\nCOMPLIANCE CHECKS:\n- Built-in compliance validators (HIPAA, PCI-DSS, SOC2)\n- Separation of duties enforcement\n- Required approvals for production changes\n- Automated compliance reports for auditors\n\nDOCUMENTATION:\n- System architecture documentation\n- Data flow diagrams\n- Security controls documentation\n- Business continuity plan\n- Disaster recovery procedures\n\nAUDITOR ACCESS:\n- Read-only audit portal\n- Pre-built reports (deployment frequency, review quality, incident correlation)\n- Export to standard formats (CSV, PDF)\n- Anonymization options for sensitive data\n```\n\n---\n\n### Question 4.3: Tool Selection & Integration Strategy\n\n#### **Code Review Platforms**\n\n**Primary: GitHub**\n- **Integration:** GitHub Apps API, webhooks for PR events\n- **Features Used:** \n  - Pull request comments API (line-specific feedback)\n  - Status checks (block merge if review fails)\n  - Required reviewers (escalate to human if needed)\n- **Alternatives:** GitLab (similar API), Bitbucket (Cloud + Server)\n\n**Plugin Approach:**\n```python\nclass CodeReviewPlatform(ABC):\n    @abstractmethod\n    def get_pr_diff(self, pr_id): pass\n    @abstractmethod\n    def post_comment(self, pr_id, comment): pass\n    @abstractmethod\n    def set_status(self, pr_id, status): pass\n\nclass GitHubPlatform(CodeReviewPlatform):\n    def get_pr_diff(self, pr_id):\n        return requests.get(f'/repos/{repo}/pulls/{pr_id}', headers=self.headers)\n    \n    def post_comment(self, pr_id, comment):\n        return requests.post(f'/repos/{repo}/pulls/{pr_id}/comments', json=comment)\n```\n\n---\n\n#### **CI/CD Systems**\n\n**Primary: GitHub Actions**\n- **Why:** Native integration with GitHub, YAML-based, generous free tier\n- **Workflow:**\n  ```yaml\n  name: AI Review & Deploy\n  on: [pull_request]\n  jobs:\n    ai-review:\n      runs-on: ubuntu-latest\n      steps:\n        - uses: actions/checkout@v3\n        - uses: company/ai-review-action@v1\n          with:\n            api-key: ${{ secrets.AI_REVIEW_API_KEY }}\n  ```\n\n**Secondary: Jenkins** (for legacy projects)\n- **Integration:** Jenkinsfile pipeline, webhook triggers\n- **Use case:** Projects already on Jenkins, on-prem deployments\n\n**Cloud-Native Options:**\n- **AWS CodePipeline:** For AWS-heavy microservices\n- **Azure DevOps:** For Azure deployments\n- **GitLab CI:** For GitLab repos\n\n---\n\n#### **Monitoring & Observability**\n\n**APM: Datadog** (Primary)\n- **Features:**\n  - Real-time metrics dashboard\n  - Custom metrics API (track deployment success, review time)\n  - Anomaly detection\n  - Log aggregation\n- **Integration:**\n  ```python\n  from datadog import statsd\n  \n  statsd.increment('ai_review.completed')\n  statsd.histogram('ai_review.duration_seconds', duration)\n  statsd.event('Deployment', f'Deployed {service} to prod', alert_type='info')\n  ```\n\n**Alternatives:**\n- **New Relic:** Similar feature set, better for .NET shops\n- **Prometheus + Grafana:** For self-hosted, Kubernetes environments\n\n**Logging: ELK Stack** (Elasticsearch, Logstash, Kibana)\n- **Use:** Centralized logs from all pipeline stages\n- **Query Example:** Find all failed deployments in last 24h with root cause\n\n---\n\n#### **Security Scanning Tools**\n\n**SAST: SonarQube** (Primary)\n- **Languages:** 25+ languages supported\n- **Integration:** Pre-commit hooks, CI/CD pipeline step\n- **Custom Rules:** Define company-specific security patterns\n\n**Dependency Scanning: Snyk**\n- **Features:** Vulnerability database, fix PRs, license compliance\n- **Integration:** GitHub App, CLI in CI/CD\n\n**Secrets Detection: GitGuardian**\n- **Features:** 350+ detectors, historical scan, auto-revoke\n- **Integration:** Pre-commit, GitHub webhooks\n\n**Alternatives:**\n- **Veracode:** Enterprise-grade, thorough but slower\n- **Checkmarx:** Good for large enterprises, compliance-heavy\n- **Semgrep:** Fast, customizable, open-source friendly\n\n---\n\n#### **Communication & Collaboration**\n\n**Chat: Slack**\n- **Integrations:**\n  - Bot for deployment notifications\n  - Interactive approvals (approve/reject from Slack)\n  - Daily digest of review stats\n- **Channels:**\n  - #deployments (all prod deployments)\n  - #ai-review-feedback (report issues)\n  - #security-alerts (critical findings)\n\n**Issue Tracking: Jira**\n- **Integration:**\n  - Auto-link PRs to Jira tickets\n  - Update ticket status on deployment\n  - Create follow-up tickets for tech debt\n- **Custom Fields:** AI review score, deployment risk level\n\n**Docs: Confluence**\n- **Use:** Runbooks, architecture docs, team coding standards\n- **AI Integration:** Fetch coding standards during review\n\n---\n\n#### **Infrastructure & Hosting**\n\n**Primary: AWS**\n- **Services:**\n  - ECS/EKS for microservices\n  - Lambda for serverless functions\n  - RDS for databases\n  - S3 for artifact storage\n  - CloudWatch for monitoring\n\n**AI/ML Platform: AWS SageMaker** (or custom)\n- **Use:** Host AI models for code review\n- **Alternatives:**\n  - **OpenAI API:** For GPT-4 based reviews (external)\n  - **Azure OpenAI:** Enterprise features, Microsoft compliance\n  - **Self-hosted LLMs:** Llama 2, CodeLlama for sensitive code\n\n**Database: PostgreSQL**\n- **Schema:** Store PR metadata, review results, deployment history\n- **Hosting:** AWS RDS (managed) or self-hosted on EC2\n\n---\n\n#### **Integration Architecture**\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                        Developer                             │\n│                   Creates Pull Request                       │\n└─────────────────┬───────────────────────────────────────────┘\n                  │\n                  v\n┌─────────────────────────────────────────────────────────────┐\n│  GitHub/GitLab (Code Review Platform)                       │\n│    - Webhook triggers on PR open/update                     │\n│    - Sends PR metadata + diff                               │\n└─────────────────┬───────────────────────────────────────────┘\n                  │\n                  v\n┌─────────────────────────────────────────────────────────────┐\n│  AI Review Pipeline (Our System)                            │\n│    ┌──────────────┐  ┌──────────────┐  ┌──────────────┐   │\n│    │ Static       │  │ Security     │  │ Test         │   │\n│    │ Analysis     │  │ Scan         │  │ Coverage     │   │\n│    │ (SonarQube)  │  │ (Snyk/Semgrep)│ │ Analysis     │   │\n│    └──────┬───────┘  └──────┬───────┘  └──────┬───────┘   │\n│           └──────────────────┼──────────────────┘           │\n│                              v                               │\n│                    ┌──────────────────┐                     │\n│                    │  AI Code Review  │                     │\n│                    │  (GPT-4 / Custom)│                     │\n│                    └────────┬─────────┘                     │\n└─────────────────────────────┼───────────────────────────────┘\n                              │\n                              v\n┌─────────────────────────────────────────────────────────────┐\n│  Post Review to GitHub                                       │\n│    - Inline comments                                         │\n│    - Overall approval/rejection                              │\n│    - Link to detailed report                                 │\n└─────────────────┬───────────────────────────────────────────┘\n                  │\n                  v  (if approved)\n┌─────────────────────────────────────────────────────────────┐\n│  CI/CD Pipeline (GitHub Actions / Jenkins)                   │\n│    - Build → Test → Deploy                                   │\n│    - Integration with cloud (AWS/Azure/GCP)                  │\n└─────────────────┬───────────────────────────────────────────┘\n                  │\n                  v\n┌─────────────────────────────────────────────────────────────┐\n│  Monitoring & Alerting (Datadog / New Relic)                │\n│    - Track deployment health                                 │\n│    - Auto-rollback on anomalies                              │\n│    - Notify via Slack                                        │\n└─────────────────────────────────────────────────────────────┘\n```\n\n**Key Integration Points:**\n1. **Webhooks:** GitHub → Our system (PR events)\n2. **APIs:** Our system ↔ SonarQube, Snyk, GitHub\n3. **Artifacts:** S3 for storing reports, logs\n4. **Metrics:** Custom metrics → Datadog\n5. **Notifications:** Our system → Slack, Jira"
  },
  {
   "cell_type": "markdown",
   "id": "584added",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}