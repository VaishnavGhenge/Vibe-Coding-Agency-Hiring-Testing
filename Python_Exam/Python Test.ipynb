{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c57bda6d",
   "metadata": {},
   "source": [
    "# Python Technical Interview - AI Agent Developer Position\n",
    "\n",
    "## Instructions\n",
    "This notebook contains 10 questions designed to test your Python skills and ability to work with AI-generated code. Each question has:\n",
    "- **Problem Description** - What you need to accomplish\n",
    "- **Code Cell** - Where you write your solution\n",
    "- **Test Cell** - Automated tests to verify your solution\n",
    "\n",
    "**Guidelines:**\n",
    "- Read each question carefully\n",
    "- You can use whatever libraries or packages\n",
    "- Some questions provide starter code, others start from scratch\n",
    "- Focus on writing clean, readable, and robust code\n",
    "- code should be able to run after clearing all outputs\n",
    "- All test cells should pass when you're done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2500ce",
   "metadata": {},
   "source": [
    "## Question 1: Debug AI-Generated Code (Lists & Logic)\n",
    "\n",
    "**Scenario:** An AI generated this code to filter products by price range, but it has several bugs. Fix the code so it works correctly.\n",
    "\n",
    "**Requirements:**\n",
    "- Filter products where price is between min_price and max_price (inclusive)\n",
    "- Handle edge cases gracefully\n",
    "- Maintain the original function signature"
   ]
  },
  {
   "cell_type": "code",
   "id": "47fab52f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T18:06:12.112147Z",
     "start_time": "2025-10-01T18:06:12.102547Z"
    }
   },
   "source": [
    "def filter_products_by_price(products, min_price, max_price):\n",
    "    \"\"\"\n",
    "    Filter products by price range.\n",
    "    \n",
    "    Args:\n",
    "        products: List of dicts with 'name' and 'price' keys\n",
    "        min_price: Minimum price (inclusive)\n",
    "        max_price: Maximum price (inclusive)\n",
    "    \n",
    "    Returns:\n",
    "        List of products within price range\n",
    "    \"\"\"\n",
    "    # Fixed: Changed to >= and <= for inclusive bounds\n",
    "    filtered = []\n",
    "    for product in products:\n",
    "        if min_price <= product['price'] <= max_price:\n",
    "            filtered.append(product)\n",
    "    return filtered\n",
    "\n",
    "# Test your solution here\n",
    "products = [\n",
    "    {'name': 'Laptop', 'price': 1000},\n",
    "    {'name': 'Mouse', 'price': 25},\n",
    "    {'name': 'Keyboard', 'price': 75},\n",
    "    {'name': 'Monitor', 'price': 300}\n",
    "]\n",
    "\n",
    "result = filter_products_by_price(products, 25, 300)\n",
    "print(\"Filtered products:\", result)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered products: [{'name': 'Mouse', 'price': 25}, {'name': 'Keyboard', 'price': 75}, {'name': 'Monitor', 'price': 300}]\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "8e8d6945",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T18:06:12.144990Z",
     "start_time": "2025-10-01T18:06:12.137793Z"
    }
   },
   "source": [
    "# Test Cell\n",
    "def test_question_1():\n",
    "    products = [\n",
    "        {'name': 'Laptop', 'price': 1000},\n",
    "        {'name': 'Mouse', 'price': 25},\n",
    "        {'name': 'Keyboard', 'price': 75},\n",
    "        {'name': 'Monitor', 'price': 300}\n",
    "    ]\n",
    "    \n",
    "    # Test inclusive bounds\n",
    "    result = filter_products_by_price(products, 25, 300)\n",
    "    expected_names = ['Mouse', 'Keyboard', 'Monitor']\n",
    "    actual_names = [p['name'] for p in result]\n",
    "    assert set(actual_names) == set(expected_names), f\"Expected {expected_names}, got {actual_names}\"\n",
    "    \n",
    "    # Test edge case - empty list\n",
    "    assert filter_products_by_price([], 0, 100) == []\n",
    "    \n",
    "    # Test no matches\n",
    "    assert filter_products_by_price(products, 2000, 3000) == []\n",
    "    \n",
    "    print(\"âœ“ Question 1 tests passed!\")\n",
    "\n",
    "test_question_1()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Question 1 tests passed!\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "2dd1256e",
   "metadata": {},
   "source": [
    "## Question 2: Fix API Integration (Error Handling)\n",
    "\n",
    "**Scenario:** This AI-generated code fetches user data from an API but lacks proper error handling. Add robust error handling and improve the code.\n",
    "\n",
    "**Requirements:**\n",
    "- Handle network timeouts\n",
    "- Handle HTTP errors (4xx, 5xx)\n",
    "- Handle JSON parsing errors\n",
    "- Return None on any error, don't let exceptions bubble up\n",
    "- Add appropriate logging"
   ]
  },
  {
   "cell_type": "code",
   "id": "964dbc8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T18:06:12.797353Z",
     "start_time": "2025-10-01T18:06:12.179200Z"
    }
   },
   "source": "import requests\nimport json\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef get_user_data(user_id):\n    \"\"\"\n    Fetch user data from API with proper error handling.\n    \n    Args:\n        user_id: User ID to fetch\n        \n    Returns:\n        dict: User data if successful, None if any error occurs\n    \"\"\"\n    url = f\"https://jsonplaceholder.typicode.com/users/{user_id}\"\n    \n    try:\n        # Set timeout to handle network delays\n        response = requests.get(url, timeout=10)\n        \n        # Handle HTTP errors (4xx, 5xx)\n        response.raise_for_status()\n        \n        # Parse JSON with error handling\n        data = response.json()\n        \n        # Validate response has data\n        if not data:\n            logger.warning(f\"Empty response for user_id: {user_id}\")\n            return None\n            \n        logger.info(f\"Successfully fetched data for user_id: {user_id}\")\n        return data\n        \n    except requests.exceptions.Timeout:\n        logger.error(f\"Timeout error fetching user {user_id}\")\n        return None\n        \n    except requests.exceptions.HTTPError as e:\n        logger.error(f\"HTTP error for user {user_id}: {e}\")\n        return None\n        \n    except requests.exceptions.RequestException as e:\n        logger.error(f\"Network error fetching user {user_id}: {e}\")\n        return None\n        \n    except json.JSONDecodeError as e:\n        logger.error(f\"JSON parsing error for user {user_id}: {e}\")\n        return None\n        \n    except Exception as e:\n        logger.error(f\"Unexpected error fetching user {user_id}: {e}\")\n        return None\n\n# Test your solution here\nuser_data = get_user_data(1)\nprint(\"User data:\", user_data)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully fetched data for user_id: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User data: {'id': 1, 'name': 'Leanne Graham', 'username': 'Bret', 'email': 'Sincere@april.biz', 'address': {'street': 'Kulas Light', 'suite': 'Apt. 556', 'city': 'Gwenborough', 'zipcode': '92998-3874', 'geo': {'lat': '-37.3159', 'lng': '81.1496'}}, 'phone': '1-770-736-8031 x56442', 'website': 'hildegard.org', 'company': {'name': 'Romaguera-Crona', 'catchPhrase': 'Multi-layered client-server neural-net', 'bs': 'harness real-time e-markets'}}\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "ba7b4f2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T18:06:13.841310Z",
     "start_time": "2025-10-01T18:06:12.820940Z"
    }
   },
   "source": [
    "# Test Cell\n",
    "import unittest.mock as mock\n",
    "\n",
    "def test_question_2():\n",
    "    # Test successful request\n",
    "    user_data = get_user_data(1)\n",
    "    assert user_data is not None\n",
    "    assert 'name' in user_data\n",
    "    \n",
    "    # Test invalid user ID\n",
    "    user_data = get_user_data(999999)\n",
    "    assert user_data is None\n",
    "    \n",
    "    # Test with mock to simulate network error\n",
    "    with mock.patch('requests.get') as mock_get:\n",
    "        mock_get.side_effect = requests.exceptions.RequestException(\"Network error\")\n",
    "        result = get_user_data(1)\n",
    "        assert result is None\n",
    "    \n",
    "    # Test with mock to simulate timeout\n",
    "    with mock.patch('requests.get') as mock_get:\n",
    "        mock_get.side_effect = requests.exceptions.Timeout(\"Timeout\")\n",
    "        result = get_user_data(1)\n",
    "        assert result is None\n",
    "    \n",
    "    print(\"âœ“ Question 2 tests passed!\")\n",
    "\n",
    "test_question_2()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully fetched data for user_id: 1\n",
      "ERROR:__main__:HTTP error for user 999999: 404 Client Error: Not Found for url: https://jsonplaceholder.typicode.com/users/999999\n",
      "ERROR:__main__:Network error fetching user 1: Network error\n",
      "ERROR:__main__:Timeout error fetching user 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Question 2 tests passed!\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "002a7f14",
   "metadata": {},
   "source": [
    "## Question 3: Code from Scratch (Data Structures)\n",
    "\n",
    "**Scenario:** Create a `TaskManager` class to manage a simple todo list.\n",
    "\n",
    "**Requirements:**\n",
    "- Add tasks with priority (1=high, 2=medium, 3=low)\n",
    "- Mark tasks as complete\n",
    "- Get tasks filtered by completion status and/or priority\n",
    "- Get task count by status"
   ]
  },
  {
   "cell_type": "code",
   "id": "2a40e690",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T18:06:13.899137Z",
     "start_time": "2025-10-01T18:06:13.892804Z"
    }
   },
   "source": "class TaskManager:\n    \"\"\"\n    A simple task manager for tracking todo items.\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize empty task manager.\"\"\"\n        self.tasks = []\n        self.next_id = 1\n    \n    def add_task(self, description, priority=2):\n        \"\"\"\n        Add a new task.\n        \n        Args:\n            description (str): Task description\n            priority (int): Priority level (1=high, 2=medium, 3=low)\n        \"\"\"\n        task = {\n            'id': self.next_id,\n            'description': description,\n            'priority': priority,\n            'completed': False\n        }\n        self.tasks.append(task)\n        self.next_id += 1\n    \n    def complete_task(self, task_id):\n        \"\"\"\n        Mark a task as complete.\n        \n        Args:\n            task_id: Unique identifier for the task\n            \n        Returns:\n            bool: True if task was found and completed, False otherwise\n        \"\"\"\n        for task in self.tasks:\n            if task['id'] == task_id:\n                task['completed'] = True\n                return True\n        return False\n    \n    def get_tasks(self, completed=None, priority=None):\n        \"\"\"\n        Get tasks filtered by status and/or priority.\n        \n        Args:\n            completed (bool, optional): Filter by completion status\n            priority (int, optional): Filter by priority level\n            \n        Returns:\n            list: List of matching tasks\n        \"\"\"\n        filtered_tasks = self.tasks\n        \n        # Filter by completed status if specified\n        if completed is not None:\n            filtered_tasks = [t for t in filtered_tasks if t['completed'] == completed]\n        \n        # Filter by priority if specified\n        if priority is not None:\n            filtered_tasks = [t for t in filtered_tasks if t['priority'] == priority]\n        \n        return filtered_tasks\n    \n    def get_task_count(self, completed=None):\n        \"\"\"\n        Get count of tasks by completion status.\n        \n        Args:\n            completed (bool, optional): Count completed (True) or pending (False) tasks\n            \n        Returns:\n            int: Number of matching tasks\n        \"\"\"\n        if completed is None:\n            return len(self.tasks)\n        \n        return len([t for t in self.tasks if t['completed'] == completed])\n\ntm = TaskManager()\ntm.add_task(\"Fix bug in login\", 1)  # High priority\ntm.add_task(\"Update documentation\", 3)  # Low priority\ntm.add_task(\"Code review\", 2)  # Medium priority\n\nprint(\"All tasks:\", len(tm.get_tasks()))\nprint(\"High priority tasks:\", len(tm.get_tasks(priority=1)))",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks: 3\n",
      "High priority tasks: 1\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "5d50272e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T18:06:13.925487Z",
     "start_time": "2025-10-01T18:06:13.922225Z"
    }
   },
   "source": [
    "# Test Cell\n",
    "def test_question_3():\n",
    "    tm = TaskManager()\n",
    "    \n",
    "    # Test adding tasks\n",
    "    tm.add_task(\"Task 1\", 1)\n",
    "    tm.add_task(\"Task 2\", 2)\n",
    "    tm.add_task(\"Task 3\", 3)\n",
    "    \n",
    "    # Test get all tasks\n",
    "    all_tasks = tm.get_tasks()\n",
    "    assert len(all_tasks) == 3\n",
    "    \n",
    "    # Test priority filtering\n",
    "    high_priority = tm.get_tasks(priority=1)\n",
    "    assert len(high_priority) == 1\n",
    "    \n",
    "    # Test task completion\n",
    "    task_id = all_tasks[0]['id']  # Assuming tasks have 'id' field\n",
    "    success = tm.complete_task(task_id)\n",
    "    assert success == True\n",
    "    \n",
    "    # Test completion filtering\n",
    "    completed_tasks = tm.get_tasks(completed=True)\n",
    "    assert len(completed_tasks) == 1\n",
    "    \n",
    "    pending_tasks = tm.get_tasks(completed=False)\n",
    "    assert len(pending_tasks) == 2\n",
    "    \n",
    "    # Test task counts\n",
    "    assert tm.get_task_count() == 3\n",
    "    assert tm.get_task_count(completed=True) == 1\n",
    "    assert tm.get_task_count(completed=False) == 2\n",
    "    \n",
    "    print(\"âœ“ Question 3 tests passed!\")\n",
    "\n",
    "test_question_3()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Question 3 tests passed!\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "1243a1dc",
   "metadata": {},
   "source": [
    "## Question 4: Optimize AI Code (Performance)\n",
    "\n",
    "**Scenario:** This AI code finds common elements between multiple lists, but it's very inefficient. Optimize it for better performance.\n",
    "\n",
    "**Requirements:**\n",
    "- Same functionality as original\n",
    "- Significantly better time complexity\n",
    "- Handle edge cases (empty lists, no common elements)"
   ]
  },
  {
   "cell_type": "code",
   "id": "2d39f526",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T18:06:13.949957Z",
     "start_time": "2025-10-01T18:06:13.946373Z"
    }
   },
   "source": "def find_common_elements_slow(lists):\n    \"\"\"\n    Find elements that appear in ALL provided lists.\n    AI-generated inefficient version - OPTIMIZE THIS!\n    \n    Args:\n        lists: List of lists to find common elements in\n        \n    Returns:\n        list: Elements that appear in all lists\n    \"\"\"\n    if not lists:\n        return []\n    \n    common = []\n    for item in lists[0]:\n        is_common = True\n        for other_list in lists[1:]:\n            found = False\n            for other_item in other_list:\n                if item == other_item:\n                    found = True\n                    break\n            if not found:\n                is_common = False\n                break\n        if is_common and item not in common:\n            common.append(item)\n    \n    return common\n\n# Optimized version - implement this\ndef find_common_elements_fast(lists):\n    \"\"\"\n    Find elements that appear in ALL provided lists.\n    Optimized version with better time complexity.\n    \n    Args:\n        lists: List of lists to find common elements in\n        \n    Returns:\n        list: Elements that appear in all lists\n    \"\"\"\n    # Handle edge cases\n    if not lists:\n        return []\n    \n    if len(lists) == 1:\n        return lists[0]\n    \n    # Convert first list to set for fast lookup\n    common_set = set(lists[0])\n    \n    # Intersect with each subsequent list\n    for lst in lists[1:]:\n        common_set = common_set.intersection(set(lst))\n        \n        # Early exit if no common elements remain\n        if not common_set:\n            return []\n    \n    # Convert back to list\n    return list(common_set)\n\n# Test both versions\ntest_lists = [\n    [1, 2, 3, 4, 5],\n    [3, 4, 5, 6, 7],\n    [4, 5, 7, 8, 9]\n]\n\nprint(\"Slow version:\", find_common_elements_slow(test_lists))\nprint(\"Fast version:\", find_common_elements_fast(test_lists))",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slow version: [4, 5]\n",
      "Fast version: [4, 5]\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "de081d21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T18:06:13.975618Z",
     "start_time": "2025-10-01T18:06:13.971815Z"
    }
   },
   "source": [
    "# Test Cell\n",
    "\n",
    "import time\n",
    "\n",
    "def test_question_4():\n",
    "    # Basic functionality test\n",
    "    test_lists = [\n",
    "        [1, 2, 3, 4, 5],\n",
    "        [3, 4, 5, 6, 7],\n",
    "        [4, 5, 7, 8, 9]\n",
    "    ]\n",
    "    \n",
    "    slow_result = find_common_elements_slow(test_lists)\n",
    "    fast_result = find_common_elements_fast(test_lists)\n",
    "    \n",
    "    assert set(slow_result) == set(fast_result), \"Results don't match\"\n",
    "    assert set(fast_result) == {4, 5}, f\"Expected {{4, 5}}, got {set(fast_result)}\"\n",
    "    \n",
    "    # Edge cases\n",
    "    assert find_common_elements_fast([]) == []\n",
    "    assert find_common_elements_fast([[1, 2], []]) == []\n",
    "    assert find_common_elements_fast([[1, 2, 3]]) == [1, 2, 3]\n",
    "    \n",
    "    # Performance test (rough)\n",
    "    large_lists = [[i for i in range(1000)] for _ in range(10)]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    find_common_elements_fast(large_lists)\n",
    "    fast_time = time.time() - start_time\n",
    "    \n",
    "    # Fast version should complete in reasonable time\n",
    "    assert fast_time < 1.0, \"Optimized version is still too slow\"\n",
    "    \n",
    "    print(\"âœ“ Question 4 tests passed!\")\n",
    "\n",
    "test_question_4()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Question 4 tests passed!\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "d7131a24",
   "metadata": {},
   "source": [
    "## Question 5: Fix Function with Edge Cases\n",
    "\n",
    "**Scenario:** This AI function calculates statistics for a list of numbers, but fails on various edge cases. Make it robust.\n",
    "\n",
    "**Requirements:**\n",
    "- Handle empty lists\n",
    "- Handle non-numeric values gracefully\n",
    "- Handle division by zero\n",
    "- Return meaningful error messages or default values"
   ]
  },
  {
   "cell_type": "code",
   "id": "d4ffe4e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T18:06:13.999604Z",
     "start_time": "2025-10-01T18:06:13.994319Z"
    }
   },
   "source": "def calculate_stats(numbers):\n    \"\"\"\n    Calculate basic statistics for a list of numbers.\n    Fixed version that handles edge cases gracefully.\n    \n    Args:\n        numbers: List of numbers\n        \n    Returns:\n        dict: Statistics including mean, median, mode, std_dev\n    \"\"\"\n    # Filter out non-numeric values and None\n    valid_numbers = []\n    for num in numbers:\n        if isinstance(num, (int, float)) and num is not None:\n            valid_numbers.append(num)\n    \n    # Handle empty list or no valid numbers\n    if not valid_numbers:\n        return {\n            'error': 'No valid numeric data',\n            'mean': None,\n            'median': None,\n            'mode': None,\n            'std_dev': None,\n            'count': 0\n        }\n    \n    # Sort for median calculation\n    sorted_nums = sorted(valid_numbers)\n    \n    # Mean\n    mean = sum(valid_numbers) / len(valid_numbers)\n    \n    # Median\n    n = len(sorted_nums)\n    if n % 2 == 0:\n        median = (sorted_nums[n//2 - 1] + sorted_nums[n//2]) / 2\n    else:\n        median = sorted_nums[n//2]\n    \n    # Mode (most frequent)\n    from collections import Counter\n    counts = Counter(valid_numbers)\n    mode = counts.most_common(1)[0][0]\n    \n    # Standard deviation\n    variance = sum((x - mean) ** 2 for x in valid_numbers) / len(valid_numbers)\n    std_dev = variance ** 0.5\n    \n    return {\n        'mean': mean,\n        'median': median,\n        'mode': mode,\n        'std_dev': std_dev,\n        'count': len(valid_numbers)\n    }\n\n# Test your solution\ntest_cases = [\n    [1, 2, 3, 4, 5],           # Normal case\n    [],                        # Empty list\n    [1],                       # Single item\n    [1, 1, 1],                # All same\n    [1, 'invalid', 3],         # Mixed types\n    [1, 2, None, 4]           # None values\n]\n\nfor i, case in enumerate(test_cases):\n    print(f\"Test case {i+1}: {case}\")\n    try:\n        result = calculate_stats(case)\n        print(f\"  Result: {result}\")\n    except Exception as e:\n        print(f\"  Error: {e}\")\n    print()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test case 1: [1, 2, 3, 4, 5]\n",
      "  Result: {'mean': 3.0, 'median': 3, 'mode': 1, 'std_dev': 1.4142135623730951, 'count': 5}\n",
      "\n",
      "Test case 2: []\n",
      "  Result: {'error': 'No valid numeric data', 'mean': None, 'median': None, 'mode': None, 'std_dev': None, 'count': 0}\n",
      "\n",
      "Test case 3: [1]\n",
      "  Result: {'mean': 1.0, 'median': 1, 'mode': 1, 'std_dev': 0.0, 'count': 1}\n",
      "\n",
      "Test case 4: [1, 1, 1]\n",
      "  Result: {'mean': 1.0, 'median': 1, 'mode': 1, 'std_dev': 0.0, 'count': 3}\n",
      "\n",
      "Test case 5: [1, 'invalid', 3]\n",
      "  Result: {'mean': 2.0, 'median': 2.0, 'mode': 1, 'std_dev': 1.0, 'count': 2}\n",
      "\n",
      "Test case 6: [1, 2, None, 4]\n",
      "  Result: {'mean': 2.3333333333333335, 'median': 2, 'mode': 1, 'std_dev': 1.247219128924647, 'count': 3}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "7b590186",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T18:06:14.021146Z",
     "start_time": "2025-10-01T18:06:14.017671Z"
    }
   },
   "source": [
    "# Test Cell\n",
    "def test_question_5():\n",
    "    # Normal case\n",
    "    result = calculate_stats([1, 2, 3, 4, 5])\n",
    "    assert result['mean'] == 3.0\n",
    "    assert result['median'] == 3.0\n",
    "    assert result['count'] == 5\n",
    "    \n",
    "    # Single item\n",
    "    result = calculate_stats([42])\n",
    "    assert result['mean'] == 42\n",
    "    assert result['median'] == 42\n",
    "    assert result['mode'] == 42\n",
    "    assert result['std_dev'] == 0\n",
    "    \n",
    "    # Empty list - should handle gracefully\n",
    "    result = calculate_stats([])\n",
    "    assert 'error' in result or all(v is None or v == 0 for v in result.values())\n",
    "    \n",
    "    # Mixed types - should handle gracefully\n",
    "    result = calculate_stats([1, 'invalid', 3])\n",
    "    assert 'error' in result or result['count'] == 2  # Only valid numbers counted\n",
    "    \n",
    "    # All same values\n",
    "    result = calculate_stats([5, 5, 5, 5])\n",
    "    assert result['mean'] == 5\n",
    "    assert result['std_dev'] == 0\n",
    "    \n",
    "    print(\"âœ“ Question 5 tests passed!\")\n",
    "\n",
    "test_question_5()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Question 5 tests passed!\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "3c594b33",
   "metadata": {},
   "source": [
    "## Question 6: Complete Partial Implementation (Pandas/Data)\n",
    "\n",
    "### Goal\n",
    "Implement `analyze_sales_data(df, group_by_column)`.\n",
    "\n",
    "### Input\n",
    "A pandas DataFrame `df` with columns:\n",
    "- `product`\n",
    "- `category`\n",
    "- `sales`\n",
    "- `profit`\n",
    "\n",
    "### Output (must match exactly)\n",
    "- Return a DataFrame **indexed by `group_by_column`** (do not reset the index).\n",
    "- Include exactly these columns (names must match):\n",
    "  - `sales_sum` â€” sum of `sales`\n",
    "  - `sales_mean` â€” mean of `sales`\n",
    "  - `profit_sum` â€” sum of `profit`\n",
    "  - `profit_mean` â€” mean of `profit`\n",
    "  - `profit_margin` â€” `profit_sum / sales_sum` (use `NaN` if `sales_sum == 0`)\n",
    "- Handle missing values: treat missing `sales` or `profit` as `0` before aggregation.\n",
    "- Sorting is **not required**.\n",
    "\n",
    "### Edge Behavior\n",
    "- If `df` is empty or `group_by_column` is missing, return an empty DataFrame with the required column names."
   ]
  },
  {
   "cell_type": "code",
   "id": "9e96db00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T18:06:14.058664Z",
     "start_time": "2025-10-01T18:06:14.049655Z"
    }
   },
   "source": "import pandas as pd\nimport numpy as np\n\ndef analyze_sales_data(df, group_by_column):\n    \"\"\"\n    Analyze sales data by grouping and calculating statistics.\n    \n    Args:\n        df: DataFrame with columns ['product', 'category', 'sales', 'profit']\n        group_by_column: Column name to group by\n        \n    Returns:\n        DataFrame with aggregated statistics indexed by group_by_column\n    \"\"\"\n    # Handle edge cases\n    if df.empty or group_by_column not in df.columns:\n        return pd.DataFrame(columns=['sales_sum', 'sales_mean', 'profit_sum', 'profit_mean', 'profit_margin'])\n    \n    # Create a copy to avoid modifying original\n    df_copy = df.copy()\n    \n    # Handle missing values: treat as 0\n    df_copy['sales'] = df_copy['sales'].fillna(0)\n    df_copy['profit'] = df_copy['profit'].fillna(0)\n    \n    # Group by the specified column\n    grouped = df_copy.groupby(group_by_column)\n    \n    # Calculate aggregations\n    result = pd.DataFrame({\n        'sales_sum': grouped['sales'].sum(),\n        'sales_mean': grouped['sales'].mean(),\n        'profit_sum': grouped['profit'].sum(),\n        'profit_mean': grouped['profit'].mean()\n    })\n    \n    # Calculate profit margin (profit/sales), handle division by zero\n    result['profit_margin'] = result.apply(\n        lambda row: row['profit_sum'] / row['sales_sum'] if row['sales_sum'] != 0 else np.nan,\n        axis=1\n    )\n    \n    # Index is already set to group_by_column from groupby\n    return result\n\n# Create sample data for testing\nsample_data = pd.DataFrame({\n    'product': ['A', 'B', 'C', 'A', 'B', 'C', 'A'],\n    'category': ['Electronics', 'Electronics', 'Clothing', 'Electronics', 'Electronics', 'Clothing', 'Electronics'],\n    'sales': [100, 200, 150, 120, np.nan, 180, 110],\n    'profit': [20, 50, 30, 25, 40, 35, 22]\n})\n\nprint(\"Sample data:\")\nprint(sample_data)\nprint(\"\\nAnalysis by product:\")\nresult = analyze_sales_data(sample_data, 'product')\nprint(result)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data:\n",
      "  product     category  sales  profit\n",
      "0       A  Electronics  100.0      20\n",
      "1       B  Electronics  200.0      50\n",
      "2       C     Clothing  150.0      30\n",
      "3       A  Electronics  120.0      25\n",
      "4       B  Electronics    NaN      40\n",
      "5       C     Clothing  180.0      35\n",
      "6       A  Electronics  110.0      22\n",
      "\n",
      "Analysis by product:\n",
      "         sales_sum  sales_mean  profit_sum  profit_mean  profit_margin\n",
      "product                                                               \n",
      "A            330.0       110.0          67    22.333333        0.20303\n",
      "B            200.0       100.0          90    45.000000        0.45000\n",
      "C            330.0       165.0          65    32.500000        0.19697\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "69a0c957",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T18:06:14.078055Z",
     "start_time": "2025-10-01T18:06:14.073439Z"
    }
   },
   "source": [
    "# Test Cell\n",
    "def test_question_6():\n",
    "    # Create test data\n",
    "    test_data = pd.DataFrame({\n",
    "        'product': ['A', 'B', 'A', 'B', 'A'],\n",
    "        'category': ['Cat1', 'Cat2', 'Cat1', 'Cat2', 'Cat1'],\n",
    "        'sales': [100, 200, 150, 300, 50],\n",
    "        'profit': [20, 40, 30, 60, 10]\n",
    "    })\n",
    "    \n",
    "    # Test grouping by product\n",
    "    result = analyze_sales_data(test_data, 'product')\n",
    "    \n",
    "    # Check structure\n",
    "    assert isinstance(result, pd.DataFrame), \"Should return DataFrame\"\n",
    "    assert len(result) == 2, \"Should have 2 groups (A and B)\"\n",
    "    \n",
    "    # Check required columns exist\n",
    "    required_cols = ['sales_sum', 'sales_mean', 'profit_sum', 'profit_mean', 'profit_margin']\n",
    "    for col in required_cols:\n",
    "        assert col in result.columns, f\"Missing column: {col}\"\n",
    "    \n",
    "    # Check calculations for product A\n",
    "    product_a = result.loc['A'] if 'A' in result.index else result[result.index == 'A'].iloc[0]\n",
    "    assert product_a['sales_sum'] == 300, \"Product A sales sum should be 300\"\n",
    "    assert product_a['profit_sum'] == 60, \"Product A profit sum should be 60\"\n",
    "    \n",
    "    print(\"âœ“ Question 6 tests passed!\")\n",
    "\n",
    "test_question_6()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Question 6 tests passed!\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "06b18d21",
   "metadata": {},
   "source": [
    "## Question 7: Refactor Messy AI Code (Clean Code)\n",
    "\n",
    "**Scenario:** This AI code works but is poorly structured and hard to maintain. Refactor it following clean code principles.\n",
    "\n",
    "**Requirements:**\n",
    "- Improve readability and maintainability\n",
    "- Add proper documentation\n",
    "- Follow naming conventions\n",
    "- Break down large functions\n",
    "- Add type hints if possible"
   ]
  },
  {
   "cell_type": "code",
   "id": "db7e3a5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T18:06:14.110016Z",
     "start_time": "2025-10-01T18:06:14.100105Z"
    }
   },
   "source": "def process_data(data):\n    \"\"\"Messy AI-generated code that works but needs refactoring - CLEAN IT UP!\"\"\"\n    result = {}\n    for item in data:\n        if 'type' in item and item['type'] == 'user':\n            if 'active' in item and item['active']:\n                if 'age' in item:\n                    if item['age'] >= 18:\n                        if 'email' in item and '@' in item['email']:\n                            category = 'adult'\n                            if item['age'] >= 65:\n                                category = 'senior'\n                            elif item['age'] >= 25:\n                                category = 'adult'\n                            else:\n                                category = 'young_adult'\n                            \n                            if category not in result:\n                                result[category] = {'count': 0, 'emails': [], 'total_age': 0}\n                            \n                            result[category]['count'] += 1\n                            result[category]['emails'].append(item['email'])\n                            result[category]['total_age'] += item['age']\n    \n    # Calculate averages\n    for cat in result:\n        result[cat]['avg_age'] = result[cat]['total_age'] / result[cat]['count']\n        del result[cat]['total_age']\n    \n    return result\n\n# Test data\ntest_data = [\n    {'type': 'user', 'active': True, 'age': 25, 'email': 'user1@test.com'},\n    {'type': 'user', 'active': True, 'age': 70, 'email': 'user2@test.com'},\n    {'type': 'user', 'active': False, 'age': 30, 'email': 'user3@test.com'},\n    {'type': 'admin', 'active': True, 'age': 35, 'email': 'admin@test.com'},\n    {'type': 'user', 'active': True, 'age': 20, 'email': 'invalid-email'},\n    {'type': 'user', 'active': True, 'age': 40, 'email': 'user4@test.com'},\n]\n\n# Your refactored version should produce the same results\noriginal_result = process_data(test_data)\nprint(\"Original result:\", original_result)\n\n# Refactored version with clean code principles\ndef is_valid_user(user: dict) -> bool:\n    \"\"\"\n    Check if user meets basic validation criteria.\n    \n    Args:\n        user: User dictionary to validate\n        \n    Returns:\n        bool: True if user is valid, False otherwise\n    \"\"\"\n    return (\n        user.get('type') == 'user' and\n        user.get('active', False) and\n        user.get('age', 0) >= 18 and\n        is_valid_email(user.get('email', ''))\n    )\n\ndef is_valid_email(email: str) -> bool:\n    \"\"\"\n    Basic email validation.\n    \n    Args:\n        email: Email string to validate\n        \n    Returns:\n        bool: True if email contains '@', False otherwise\n    \"\"\"\n    return isinstance(email, str) and '@' in email\n\ndef get_age_category(age: int) -> str:\n    \"\"\"\n    Determine age category for a given age.\n    \n    Args:\n        age: User's age\n        \n    Returns:\n        str: Age category ('senior', 'adult', or 'young_adult')\n    \"\"\"\n    if age >= 65:\n        return 'senior'\n    elif age >= 25:\n        return 'adult'\n    else:\n        return 'young_adult'\n\ndef initialize_category_stats() -> dict:\n    \"\"\"\n    Create empty statistics dictionary for a category.\n    \n    Returns:\n        dict: Empty stats with count, emails, and total_age\n    \"\"\"\n    return {\n        'count': 0,\n        'emails': [],\n        'total_age': 0\n    }\n\ndef add_user_to_category(category_stats: dict, user: dict) -> None:\n    \"\"\"\n    Add a user to category statistics.\n    \n    Args:\n        category_stats: Statistics dictionary for the category\n        user: User dictionary to add\n    \"\"\"\n    category_stats['count'] += 1\n    category_stats['emails'].append(user['email'])\n    category_stats['total_age'] += user['age']\n\ndef calculate_average_ages(results: dict) -> None:\n    \"\"\"\n    Calculate average age for each category and remove total_age.\n    \n    Args:\n        results: Results dictionary to update in-place\n    \"\"\"\n    for category in results:\n        results[category]['avg_age'] = results[category]['total_age'] / results[category]['count']\n        del results[category]['total_age']\n\ndef process_user_data_clean(data: list) -> dict:\n    \"\"\"\n    Process user data and categorize by age groups.\n    \n    Filters for active adult users with valid emails, then groups them\n    into age categories (young_adult: 18-24, adult: 25-64, senior: 65+).\n    \n    Args:\n        data: List of user dictionaries\n        \n    Returns:\n        dict: Statistics for each age category including count, emails, and avg_age\n    \"\"\"\n    results = {}\n    \n    for user in data:\n        # Skip invalid users\n        if not is_valid_user(user):\n            continue\n        \n        # Determine age category\n        category = get_age_category(user['age'])\n        \n        # Initialize category if needed\n        if category not in results:\n            results[category] = initialize_category_stats()\n        \n        # Add user to category\n        add_user_to_category(results[category], user)\n    \n    # Calculate average ages\n    calculate_average_ages(results)\n    \n    return results\n\n# Test both versions\nclean_result = process_user_data_clean(test_data)\nprint(\"Clean result:\", clean_result)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original result: {'adult': {'count': 2, 'emails': ['user1@test.com', 'user4@test.com'], 'avg_age': 32.5}, 'senior': {'count': 1, 'emails': ['user2@test.com'], 'avg_age': 70.0}}\n",
      "Clean result: {'adult': {'count': 2, 'emails': ['user1@test.com', 'user4@test.com'], 'avg_age': 32.5}, 'senior': {'count': 1, 'emails': ['user2@test.com'], 'avg_age': 70.0}}\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "93e1ec59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T18:06:14.126299Z",
     "start_time": "2025-10-01T18:06:14.122839Z"
    }
   },
   "source": [
    "# Test Cell\n",
    "def test_question_7():\n",
    "    test_data = [\n",
    "        {'type': 'user', 'active': True, 'age': 25, 'email': 'user1@test.com'},\n",
    "        {'type': 'user', 'active': True, 'age': 70, 'email': 'user2@test.com'},\n",
    "        {'type': 'user', 'active': False, 'age': 30, 'email': 'user3@test.com'},\n",
    "        {'type': 'user', 'active': True, 'age': 20, 'email': 'user4@test.com'},\n",
    "    ]\n",
    "    \n",
    "    original_result = process_data(test_data)\n",
    "    clean_result = process_user_data_clean(test_data)\n",
    "    \n",
    "    # Results should be functionally equivalent\n",
    "    assert set(original_result.keys()) == set(clean_result.keys()), \"Categories don't match\"\n",
    "    \n",
    "    for category in original_result:\n",
    "        assert original_result[category]['count'] == clean_result[category]['count'], f\"Count mismatch for {category}\"\n",
    "        assert abs(original_result[category]['avg_age'] - clean_result[category]['avg_age']) < 0.01, f\"Average age mismatch for {category}\"\n",
    "    \n",
    "    print(\"âœ“ Question 7 tests passed!\")\n",
    "\n",
    "test_question_7()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Question 7 tests passed!\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "id": "9bf968be",
   "metadata": {},
   "source": [
    "## Question 8: Debug Complex Logic (Algorithms)\n",
    "\n",
    "**Scenario:** This AI implementation of binary search has subtle bugs. Find and fix all the issues.\n",
    "\n",
    "**Requirements:**\n",
    "- Fix the binary search algorithm\n",
    "- Handle edge cases properly\n",
    "- Maintain O(log n) time complexity\n",
    "- Return correct index or -1 if not found"
   ]
  },
  {
   "cell_type": "code",
   "id": "b3b759b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T18:06:14.147921Z",
     "start_time": "2025-10-01T18:06:14.144952Z"
    }
   },
   "source": "def binary_search_buggy(arr, target):\n    \"\"\"\n    Binary search implementation - FIXED VERSION\n    \n    Args:\n        arr: Sorted list of integers\n        target: Value to search for\n        \n    Returns:\n        int: Index of target if found, -1 otherwise\n    \"\"\"\n    left = 0\n    right = len(arr) - 1  # Fixed: was len(arr), should be len(arr) - 1\n    \n    while left <= right:  # Fixed: was left < right, should be left <= right\n        mid = (left + right) // 2\n        \n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1  # Fixed: was left = mid, should be left = mid + 1\n        else:\n            right = mid - 1  # Fixed: was right = mid, should be right = mid - 1\n    \n    return -1\n\n# Test cases\ntest_arrays = [\n    ([1, 3, 5, 7, 9, 11], 7),    # Should find at index 3\n    ([1, 3, 5, 7, 9, 11], 1),    # Should find at index 0\n    ([1, 3, 5, 7, 9, 11], 11),   # Should find at index 5\n    ([1, 3, 5, 7, 9, 11], 6),    # Should return -1\n    ([5], 5),                     # Single element found\n    ([5], 3),                     # Single element not found\n    ([], 5),                      # Empty array\n]\n\nfor arr, target in test_arrays:\n    result = binary_search_buggy(arr, target)\n    print(f\"Searching for {target} in {arr}: {result}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for 7 in [1, 3, 5, 7, 9, 11]: 3\n",
      "Searching for 1 in [1, 3, 5, 7, 9, 11]: 0\n",
      "Searching for 11 in [1, 3, 5, 7, 9, 11]: 5\n",
      "Searching for 6 in [1, 3, 5, 7, 9, 11]: -1\n",
      "Searching for 5 in [5]: 0\n",
      "Searching for 3 in [5]: -1\n",
      "Searching for 5 in []: -1\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "51e288e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T18:06:14.172293Z",
     "start_time": "2025-10-01T18:06:14.168791Z"
    }
   },
   "source": [
    "# Test Cell\n",
    "def test_question_8():\n",
    "    # Test cases with expected results\n",
    "    test_cases = [\n",
    "        ([1, 3, 5, 7, 9, 11], 7, 3),      # Found at index 3\n",
    "        ([1, 3, 5, 7, 9, 11], 1, 0),      # Found at index 0\n",
    "        ([1, 3, 5, 7, 9, 11], 11, 5),     # Found at index 5\n",
    "        ([1, 3, 5, 7, 9, 11], 6, -1),     # Not found\n",
    "        ([1, 3, 5, 7, 9, 11], 0, -1),     # Less than min\n",
    "        ([1, 3, 5, 7, 9, 11], 12, -1),    # Greater than max\n",
    "        ([5], 5, 0),                       # Single element found\n",
    "        ([5], 3, -1),                      # Single element not found\n",
    "        ([], 5, -1),                       # Empty array\n",
    "    ]\n",
    "    \n",
    "    for arr, target, expected in test_cases:\n",
    "        result = binary_search_buggy(arr, target)\n",
    "        assert result == expected, f\"Failed for {target} in {arr}: expected {expected}, got {result}\"\n",
    "    \n",
    "    # Test that it actually uses binary search (check performance)\n",
    "    large_array = list(range(0, 10000, 2))  # [0, 2, 4, 6, ..., 9998]\n",
    "    result = binary_search_buggy(large_array, 5000)\n",
    "    assert result == 2500, \"Should find 5000 at index 2500\"\n",
    "    \n",
    "    print(\"âœ“ Question 8 tests passed!\")\n",
    "\n",
    "test_question_8()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Question 8 tests passed!\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "id": "495ecefa",
   "metadata": {},
   "source": [
    "## Question 9: Add Missing Functionality\n",
    "\n",
    "**Scenario:** This AI code provides a basic cache implementation but is missing several key features. Add the missing functionality to make it production-ready.\n",
    "\n",
    "**Requirements:**\n",
    "- Add TTL (time-to-live) support for automatic expiration\n",
    "- Add size limit with LRU (Least Recently Used) eviction\n",
    "- Add cache statistics tracking (hits, misses, evictions)\n",
    "- Add methods for cache management (clear, size, cleanup)\n",
    "- Handle thread safety considerations"
   ]
  },
  {
   "cell_type": "code",
   "id": "2cafdbc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T18:06:16.409827Z",
     "start_time": "2025-10-01T18:06:14.188158Z"
    }
   },
   "source": "import time\nfrom typing import Any, Optional, Dict\nfrom collections import OrderedDict\n\nclass SimpleCache:\n    \"\"\"\n    Enhanced cache with TTL, LRU eviction, and statistics tracking.\n    \"\"\"\n    \n    def __init__(self, max_size: int = 100, default_ttl: Optional[int] = None):\n        \"\"\"\n        Initialize cache with size limit and default TTL.\n        \n        Args:\n            max_size: Maximum number of items to store\n            default_ttl: Default time-to-live in seconds (None = no expiration)\n        \"\"\"\n        self.max_size = max_size\n        self.default_ttl = default_ttl\n        \n        # OrderedDict for LRU tracking (maintains insertion/access order)\n        self._data = OrderedDict()\n        \n        # TTL tracking: key -> expiration timestamp\n        self._expiration_times = {}\n        \n        # Statistics\n        self._stats = {\n            'hits': 0,\n            'misses': 0,\n            'evictions': 0\n        }\n    \n    def get(self, key: str) -> Optional[Any]:\n        \"\"\"\n        Get value from cache.\n        \n        Args:\n            key: Cache key\n            \n        Returns:\n            Cached value or None if not found/expired\n        \"\"\"\n        # Check if key exists\n        if key not in self._data:\n            self._stats['misses'] += 1\n            return None\n        \n        # Check if expired\n        if self._is_expired(key):\n            self.delete(key)\n            self._stats['misses'] += 1\n            return None\n        \n        # Update LRU order (move to end = most recently used)\n        self._data.move_to_end(key)\n        \n        # Update statistics\n        self._stats['hits'] += 1\n        \n        return self._data[key]\n    \n    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> None:\n        \"\"\"\n        Set value in cache.\n        \n        Args:\n            key: Cache key\n            value: Value to cache\n            ttl: Time-to-live in seconds (overrides default)\n        \"\"\"\n        # If key exists, remove it first (will re-add at end for LRU)\n        if key in self._data:\n            del self._data[key]\n            if key in self._expiration_times:\n                del self._expiration_times[key]\n        \n        # Check if cache is full and evict LRU items\n        if len(self._data) >= self.max_size:\n            self._evict_lru(1)\n        \n        # Store value\n        self._data[key] = value\n        \n        # Calculate and store expiration time if TTL provided\n        effective_ttl = ttl if ttl is not None else self.default_ttl\n        if effective_ttl is not None:\n            self._expiration_times[key] = time.time() + effective_ttl\n    \n    def delete(self, key: str) -> bool:\n        \"\"\"Delete key from cache.\"\"\"\n        if key in self._data:\n            del self._data[key]\n            if key in self._expiration_times:\n                del self._expiration_times[key]\n            return True\n        return False\n    \n    def clear(self) -> None:\n        \"\"\"Clear all items from cache.\"\"\"\n        self._data.clear()\n        self._expiration_times.clear()\n    \n    def size(self) -> int:\n        \"\"\"Return current number of items in cache.\"\"\"\n        return len(self._data)\n    \n    def get_stats(self) -> Dict[str, int]:\n        \"\"\"\n        Get cache statistics.\n        \n        Returns:\n            Dict with keys: hits, misses, evictions, current_size\n        \"\"\"\n        return {\n            'hits': self._stats['hits'],\n            'misses': self._stats['misses'],\n            'evictions': self._stats['evictions'],\n            'current_size': len(self._data)\n        }\n    \n    def cleanup_expired(self) -> int:\n        \"\"\"\n        Remove expired items from cache.\n        \n        Returns:\n            Number of items removed\n        \"\"\"\n        expired_keys = []\n        current_time = time.time()\n        \n        for key, expiration_time in self._expiration_times.items():\n            if current_time >= expiration_time:\n                expired_keys.append(key)\n        \n        for key in expired_keys:\n            self.delete(key)\n        \n        return len(expired_keys)\n    \n    def _evict_lru(self, count: int = 1) -> int:\n        \"\"\"\n        Evict least recently used items.\n        \n        Args:\n            count: Number of items to evict\n            \n        Returns:\n            Number of items actually evicted\n        \"\"\"\n        evicted = 0\n        for _ in range(count):\n            if not self._data:\n                break\n            \n            # Remove first item (least recently used in OrderedDict)\n            key, _ = self._data.popitem(last=False)\n            \n            if key in self._expiration_times:\n                del self._expiration_times[key]\n            \n            evicted += 1\n            self._stats['evictions'] += 1\n        \n        return evicted\n    \n    def _is_expired(self, key: str) -> bool:\n        \"\"\"Check if a cache entry has expired.\"\"\"\n        if key not in self._expiration_times:\n            return False\n        \n        return time.time() >= self._expiration_times[key]\n\n# Test your enhanced implementation\nif __name__ == \"__main__\":\n    # Test TTL functionality\n    cache = SimpleCache(max_size=3, default_ttl=1)  # 1 second TTL\n    \n    print(\"=== Testing TTL ===\")\n    cache.set(\"temp_key\", \"temp_value\")\n    print(f\"Immediately after set: {cache.get('temp_key')}\")\n    time.sleep(1.1)\n    print(f\"After TTL expired: {cache.get('temp_key')}\")\n    \n    print(\"\\n=== Testing Size Limits & LRU ===\")\n    cache.clear()\n    cache.set(\"a\", 1, ttl=None)  # No expiration\n    cache.set(\"b\", 2, ttl=None)\n    cache.set(\"c\", 3, ttl=None)\n    print(f\"Cache size after adding 3 items: {cache.size()}\")\n    \n    # Access 'a' to make it recently used\n    cache.get(\"a\")\n    \n    # Add 'd' which should evict 'b' (least recently used)\n    cache.set(\"d\", 4, ttl=None)\n    print(f\"After adding 'd': a={cache.get('a')}, b={cache.get('b')}, c={cache.get('c')}, d={cache.get('d')}\")\n    \n    print(\"\\n=== Testing Statistics ===\")\n    stats = cache.get_stats()\n    print(f\"Cache statistics: {stats}\")\n    \n    print(\"\\n=== Testing Cleanup ===\")\n    cache.set(\"expire_me\", \"value\", ttl=1)\n    time.sleep(1.1)\n    removed_count = cache.cleanup_expired()\n    print(f\"Expired items removed: {removed_count}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing TTL ===\n",
      "Immediately after set: temp_value\n",
      "After TTL expired: None\n",
      "\n",
      "=== Testing Size Limits & LRU ===\n",
      "Cache size after adding 3 items: 3\n",
      "After adding 'd': a=1, b=None, c=3, d=4\n",
      "\n",
      "=== Testing Statistics ===\n",
      "Cache statistics: {'hits': 5, 'misses': 2, 'evictions': 1, 'current_size': 3}\n",
      "\n",
      "=== Testing Cleanup ===\n",
      "Expired items removed: 3\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "7bf0f585",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T18:06:18.665431Z",
     "start_time": "2025-10-01T18:06:16.451099Z"
    }
   },
   "source": [
    "# Test Cell \n",
    "import time\n",
    "\n",
    "def test_question_9():\n",
    "    print(\"Testing enhanced cache implementation...\")\n",
    "    \n",
    "    # Test 1: Basic functionality\n",
    "    cache = SimpleCache(max_size=3, default_ttl=60)\n",
    "    \n",
    "    cache.set(\"key1\", \"value1\")\n",
    "    cache.set(\"key2\", \"value2\")\n",
    "    \n",
    "    assert cache.get(\"key1\") == \"value1\", \"Basic get/set failed\"\n",
    "    assert cache.get(\"key2\") == \"value2\", \"Basic get/set failed\"\n",
    "    assert cache.size() == 2, f\"Expected size 2, got {cache.size()}\"\n",
    "    \n",
    "    # Test 2: TTL expiration\n",
    "    cache.clear()\n",
    "    cache.set(\"ttl_key\", \"ttl_value\", ttl=1)  # 1 second TTL\n",
    "    assert cache.get(\"ttl_key\") == \"ttl_value\", \"TTL key should be accessible immediately\"\n",
    "    \n",
    "    time.sleep(1.1)  # Wait for expiration\n",
    "    assert cache.get(\"ttl_key\") is None, \"TTL key should be expired and return None\"\n",
    "    \n",
    "    # Test 3: Size limits and LRU eviction\n",
    "    cache.clear()\n",
    "    cache.set(\"a\", 1)\n",
    "    cache.set(\"b\", 2) \n",
    "    cache.set(\"c\", 3)  # Cache is now full (max_size=3)\n",
    "    \n",
    "    # Access 'a' to make it recently used\n",
    "    cache.get(\"a\")\n",
    "    \n",
    "    # Add 'd' which should evict 'b' (least recently used)\n",
    "    cache.set(\"d\", 4)\n",
    "    \n",
    "    assert cache.get(\"a\") == 1, \"Recently used 'a' should not be evicted\"\n",
    "    assert cache.get(\"b\") is None, \"Least recently used 'b' should be evicted\"\n",
    "    assert cache.get(\"c\") == 3, \"'c' should still be in cache\"\n",
    "    assert cache.get(\"d\") == 4, \"Newly added 'd' should be in cache\"\n",
    "    assert cache.size() == 3, \"Cache size should remain at max_size\"\n",
    "    \n",
    "    # Test 4: Statistics tracking\n",
    "    cache.clear()\n",
    "    cache.set(\"stat_key\", \"stat_value\")\n",
    "    cache.get(\"stat_key\")  # Hit\n",
    "    cache.get(\"nonexistent\")  # Miss\n",
    "    \n",
    "    stats = cache.get_stats()\n",
    "    required_stats = [\"hits\", \"misses\", \"evictions\", \"current_size\"]\n",
    "    for stat in required_stats:\n",
    "        assert stat in stats, f\"Missing statistic: {stat}\"\n",
    "    \n",
    "    assert stats[\"hits\"] > 0, \"Should have recorded hits\"\n",
    "    assert stats[\"misses\"] > 0, \"Should have recorded misses\"\n",
    "    assert stats[\"current_size\"] == 1, \"Should track current size\"\n",
    "    \n",
    "    # Test 5: Manual cleanup\n",
    "    cache.clear()\n",
    "    cache.set(\"expire1\", \"value1\", ttl=1)\n",
    "    cache.set(\"expire2\", \"value2\", ttl=1)\n",
    "    cache.set(\"keep\", \"value3\", ttl=None)  # No expiration\n",
    "    \n",
    "    time.sleep(1.1)  # Wait for expiration\n",
    "    removed_count = cache.cleanup_expired()\n",
    "    \n",
    "    assert removed_count == 2, f\"Should have removed 2 expired items, removed {removed_count}\"\n",
    "    assert cache.get(\"keep\") == \"value3\", \"Non-expiring item should remain\"\n",
    "    assert cache.size() == 1, \"Only one item should remain after cleanup\"\n",
    "    \n",
    "    # Test 6: Edge cases\n",
    "    cache.clear()\n",
    "    assert cache.size() == 0, \"Cache should be empty after clear\"\n",
    "    assert cache.get(\"nonexistent\") is None, \"Getting non-existent key should return None\"\n",
    "    assert cache.delete(\"nonexistent\") == False, \"Deleting non-existent key should return False\"\n",
    "    \n",
    "    # Test delete functionality\n",
    "    cache.set(\"delete_me\", \"value\")\n",
    "    assert cache.delete(\"delete_me\") == True, \"Deleting existing key should return True\"\n",
    "    assert cache.get(\"delete_me\") is None, \"Deleted key should not be accessible\"\n",
    "    \n",
    "    print(\"âœ“ All Question 9 tests passed!\")\n",
    "\n",
    "test_question_9()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing enhanced cache implementation...\n",
      "âœ“ All Question 9 tests passed!\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "id": "1391df2c",
   "metadata": {},
   "source": [
    "## Question 10: Integration Challenge (Multiple Components)\n",
    "\n",
    "**Scenario:** You have three separate AI-generated modules that need to work together in a data processing pipeline, but they have interface mismatches and compatibility issues. Your job is to create the integration layer that makes them work together seamlessly.\n",
    "\n",
    "**Requirements:**\n",
    "- Create adapter/wrapper functions to handle data format conversions\n",
    "- Build a unified pipeline that chains all three components\n",
    "- Add comprehensive error handling for the integration\n",
    "- Handle edge cases and invalid data gracefully\n",
    "- Create helper functions for data transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a4a060da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T18:06:18.717443Z",
     "start_time": "2025-10-01T18:06:18.701867Z"
    }
   },
   "source": "import json\nfrom typing import List, Dict, Any, Tuple, Optional, Union\n\n# Component 1: Data Processor (returns dict with specific structure)\nclass DataProcessor:\n    \"\"\"AI Component 1 - processes raw data and returns structured dict\"\"\"\n    \n    def process_data(self, raw_data: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"Process raw data and return structured dict.\"\"\"\n        if not isinstance(raw_data, list):\n            raise ValueError(\"Expected list input\")\n        \n        result = {\n            'total_items': len(raw_data),\n            'processed_items': [],\n            'metadata': {'processing_time': 0.1, 'timestamp': '2024-01-01T12:00:00Z'}\n        }\n        \n        for item in raw_data:\n            if isinstance(item, dict) and 'value' in item:\n                result['processed_items'].append({\n                    'id': item.get('id', 'unknown'),\n                    'processed_value': item['value'] * 2,\n                    'original_value': item['value'],\n                    'status': 'processed'\n                })\n            else:\n                result['processed_items'].append({\n                    'id': 'error',\n                    'processed_value': 0,\n                    'original_value': None,\n                    'status': 'failed'\n                })\n        \n        return result\n\n# Component 2: Analytics Engine (expects JSON string, returns tuple)\nclass AnalyticsEngine:\n    \"\"\"AI Component 2 - performs analytics on data, expects JSON string input\"\"\"\n    \n    def analyze(self, json_data_string: str) -> Tuple[Optional[str], Union[Dict[str, float], str]]:\n        \"\"\"Analyze data from JSON string, return (summary, metrics) tuple.\"\"\"\n        try:\n            data = json.loads(json_data_string)\n        except json.JSONDecodeError:\n            return None, \"Invalid JSON format\"\n        \n        if not isinstance(data, dict) or 'processed_items' not in data:\n            return None, \"Missing processed_items in data structure\"\n        \n        items = data['processed_items']\n        if not isinstance(items, list):\n            return None, \"processed_items must be a list\"\n        \n        # Extract numeric values for analysis\n        values = []\n        failed_count = 0\n        \n        for item in items:\n            if isinstance(item, dict) and item.get('status') == 'processed':\n                if 'processed_value' in item and isinstance(item['processed_value'], (int, float)):\n                    values.append(item['processed_value'])\n            else:\n                failed_count += 1\n        \n        if not values:\n            return None, \"No valid numeric data found for analysis\"\n        \n        summary = f\"Analyzed {len(items)} items ({len(values)} successful, {failed_count} failed)\"\n        metrics = {\n            'avg_value': sum(values) / len(values),\n            'max_value': max(values),\n            'min_value': min(values),\n            'total_value': sum(values),\n            'success_rate': len(values) / len(items) if items else 0.0\n        }\n        \n        return summary, metrics\n\n# Component 3: Report Generator (expects list of tuples, returns formatted string)\nclass ReportGenerator:\n    \"\"\"AI Component 3 - generates reports from analytics results\"\"\"\n    \n    def generate_report(self, analytics_results_list: List[Tuple[Optional[str], Union[Dict, str]]]) -> str:\n        \"\"\"Generate report from list of (summary, metrics) tuples.\"\"\"\n        if not isinstance(analytics_results_list, list):\n            return \"Error: Expected list input for report generation\"\n        \n        if not analytics_results_list:\n            return \"Error: No data provided for report generation\"\n        \n        report_lines = [\n            \"=\" * 50,\n            \"           ANALYSIS REPORT\",\n            \"=\" * 50\n        ]\n        \n        for i, result in enumerate(analytics_results_list):\n            if not isinstance(result, tuple) or len(result) != 2:\n                report_lines.append(f\"\\nSection {i+1}: Invalid data format - expected (summary, metrics) tuple\")\n                continue\n            \n            summary, metrics = result\n            \n            if summary is None:\n                report_lines.append(f\"\\nSection {i+1}: Analysis failed\")\n                report_lines.append(f\"  Error: {metrics}\")\n                continue\n            \n            report_lines.append(f\"\\nSection {i+1}: {summary}\")\n            \n            if isinstance(metrics, dict):\n                report_lines.append(\"  Metrics:\")\n                for key, value in metrics.items():\n                    if isinstance(value, float):\n                        report_lines.append(f\"    {key}: {value:.2f}\")\n                    else:\n                        report_lines.append(f\"    {key}: {value}\")\n            else:\n                report_lines.append(f\"  Metrics: {metrics}\")\n        \n        report_lines.append(\"\\n\" + \"=\" * 50)\n        return \"\\n\".join(report_lines)\n\n# Integration functions implementation\n\ndef dict_to_json_adapter(data_dict: Dict[str, Any]) -> str:\n    \"\"\"\n    Convert dictionary to JSON string for AnalyticsEngine.\n    \n    Args:\n        data_dict: Dictionary from DataProcessor\n        \n    Returns:\n        JSON string suitable for AnalyticsEngine\n    \"\"\"\n    try:\n        return json.dumps(data_dict)\n    except (TypeError, ValueError) as e:\n        # Return empty valid JSON structure if conversion fails\n        return json.dumps({'processed_items': []})\n\ndef validate_and_clean_raw_data(raw_data: Any) -> List[Dict[str, Any]]:\n    \"\"\"\n    Validate and clean raw input data.\n    \n    Args:\n        raw_data: Input data of any type\n        \n    Returns:\n        Cleaned list of dictionaries\n    \"\"\"\n    # Handle non-list inputs\n    if not isinstance(raw_data, list):\n        return []\n    \n    cleaned = []\n    for item in raw_data:\n        # Only keep dictionary items\n        if isinstance(item, dict):\n            cleaned.append(item)\n    \n    return cleaned\n\ndef integrated_pipeline(raw_data_list: List[Any]) -> str:\n    \"\"\"\n    Integrate all three components to process data end-to-end.\n    \n    This function:\n    1. Validates and cleans each raw dataset\n    2. Processes each dataset through DataProcessor\n    3. Converts results to format expected by AnalyticsEngine\n    4. Runs analytics on each processed dataset\n    5. Collects all analytics results\n    6. Generates final report using ReportGenerator\n    7. Handles all errors gracefully\n    \n    Args:\n        raw_data_list: List of raw data sets to process\n        \n    Returns:\n        str: Final report combining all analyses\n    \"\"\"\n    # Initialize components\n    processor = DataProcessor()\n    analytics = AnalyticsEngine()\n    reporter = ReportGenerator()\n    \n    # Handle empty input\n    if not raw_data_list:\n        return reporter.generate_report([])\n    \n    # Process each dataset\n    analytics_results = []\n    \n    for raw_data in raw_data_list:\n        try:\n            # Step 1: Validate and clean the data\n            cleaned_data = validate_and_clean_raw_data(raw_data)\n            \n            # Step 2: Process through DataProcessor\n            try:\n                processed_data = processor.process_data(cleaned_data)\n            except Exception as e:\n                # If processing fails, add error result\n                analytics_results.append((None, f\"Data processing error: {str(e)}\"))\n                continue\n            \n            # Step 3: Convert to JSON for AnalyticsEngine\n            json_data = dict_to_json_adapter(processed_data)\n            \n            # Step 4: Run analytics\n            try:\n                analysis_result = analytics.analyze(json_data)\n                analytics_results.append(analysis_result)\n            except Exception as e:\n                # If analysis fails, add error result\n                analytics_results.append((None, f\"Analysis error: {str(e)}\"))\n                \n        except Exception as e:\n            # Catch any unexpected errors\n            analytics_results.append((None, f\"Unexpected error: {str(e)}\"))\n    \n    # Step 5: Generate final report\n    try:\n        return reporter.generate_report(analytics_results)\n    except Exception as e:\n        return f\"Error generating report: {str(e)}\"\n\ndef create_sample_data() -> List[List[Dict[str, Any]]]:\n    \"\"\"Create sample test data for the pipeline.\"\"\"\n    return [\n        # Dataset 1: Normal data\n        [\n            {'id': 'A1', 'value': 10},\n            {'id': 'A2', 'value': 20},\n            {'id': 'A3', 'value': 15}\n        ],\n        # Dataset 2: Smaller dataset\n        [\n            {'id': 'B1', 'value': 5},\n            {'id': 'B2', 'value': 25}\n        ],\n        # Dataset 3: Mixed data with issues\n        [\n            {'id': 'C1', 'value': 30},\n            {'id': 'C2'},  # Missing value\n            {'value': 40},  # Missing id\n            {'id': 'C4', 'value': 'invalid'},  # Invalid value type\n        ]\n    ]\n\n# Test the integration\nif __name__ == \"__main__\":\n    print(\"Testing component integration...\")\n    \n    # Test individual components first\n    print(\"\\n=== Testing Individual Components ===\")\n    \n    processor = DataProcessor()\n    analytics = AnalyticsEngine()\n    reporter = ReportGenerator()\n    \n    # Test DataProcessor\n    test_data = [{'id': 'test', 'value': 10}]\n    processed = processor.process_data(test_data)\n    print(f\"DataProcessor output: {processed}\")\n    \n    # Test AnalyticsEngine\n    json_data = json.dumps(processed)\n    analysis_result = analytics.analyze(json_data)\n    print(f\"AnalyticsEngine output: {analysis_result}\")\n    \n    # Test ReportGenerator\n    report = reporter.generate_report([analysis_result])\n    print(f\"ReportGenerator output:\\n{report}\")\n    \n    print(\"\\n=== Testing Integrated Pipeline ===\")\n    \n    # Test full pipeline\n    sample_datasets = create_sample_data()\n    \n    try:\n        final_report = integrated_pipeline(sample_datasets)\n        print(\"Integration successful!\")\n        print(final_report)\n    except Exception as e:\n        print(f\"Integration failed: {e}\")\n        import traceback\n        traceback.print_exc()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing component integration...\n",
      "\n",
      "=== Testing Individual Components ===\n",
      "DataProcessor output: {'total_items': 1, 'processed_items': [{'id': 'test', 'processed_value': 20, 'original_value': 10, 'status': 'processed'}], 'metadata': {'processing_time': 0.1, 'timestamp': '2024-01-01T12:00:00Z'}}\n",
      "AnalyticsEngine output: ('Analyzed 1 items (1 successful, 0 failed)', {'avg_value': 20.0, 'max_value': 20, 'min_value': 20, 'total_value': 20, 'success_rate': 1.0})\n",
      "ReportGenerator output:\n",
      "==================================================\n",
      "           ANALYSIS REPORT\n",
      "==================================================\n",
      "\n",
      "Section 1: Analyzed 1 items (1 successful, 0 failed)\n",
      "  Metrics:\n",
      "    avg_value: 20.00\n",
      "    max_value: 20\n",
      "    min_value: 20\n",
      "    total_value: 20\n",
      "    success_rate: 1.00\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== Testing Integrated Pipeline ===\n",
      "Integration successful!\n",
      "==================================================\n",
      "           ANALYSIS REPORT\n",
      "==================================================\n",
      "\n",
      "Section 1: Analyzed 3 items (3 successful, 0 failed)\n",
      "  Metrics:\n",
      "    avg_value: 30.00\n",
      "    max_value: 40\n",
      "    min_value: 20\n",
      "    total_value: 90\n",
      "    success_rate: 1.00\n",
      "\n",
      "Section 2: Analyzed 2 items (2 successful, 0 failed)\n",
      "  Metrics:\n",
      "    avg_value: 30.00\n",
      "    max_value: 50\n",
      "    min_value: 10\n",
      "    total_value: 60\n",
      "    success_rate: 1.00\n",
      "\n",
      "Section 3: Analyzed 4 items (2 successful, 1 failed)\n",
      "  Metrics:\n",
      "    avg_value: 70.00\n",
      "    max_value: 80\n",
      "    min_value: 60\n",
      "    total_value: 140\n",
      "    success_rate: 0.50\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "id": "34c3064b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T18:06:18.740838Z",
     "start_time": "2025-10-01T18:06:18.734012Z"
    }
   },
   "source": [
    "# Test Cell\n",
    "def test_question_10():\n",
    "    print(\"Testing integrated pipeline...\")\n",
    "    \n",
    "    # Test 1: Individual component functionality\n",
    "    processor = DataProcessor()\n",
    "    analytics = AnalyticsEngine()\n",
    "    reporter = ReportGenerator()\n",
    "    \n",
    "    # Test DataProcessor\n",
    "    test_data = [{'id': 'test1', 'value': 10}, {'id': 'test2', 'value': 20}]\n",
    "    processed = processor.process_data(test_data)\n",
    "    \n",
    "    assert isinstance(processed, dict), \"DataProcessor should return dict\"\n",
    "    assert 'total_items' in processed, \"Missing total_items in processed data\"\n",
    "    assert 'processed_items' in processed, \"Missing processed_items in processed data\"\n",
    "    assert processed['total_items'] == 2, \"Should count items correctly\"\n",
    "    \n",
    "    # Test AnalyticsEngine\n",
    "    json_data = json.dumps(processed)\n",
    "    summary, metrics = analytics.analyze(json_data)\n",
    "    \n",
    "    assert summary is not None, \"Analytics should return valid summary\"\n",
    "    assert isinstance(metrics, dict), \"Analytics should return metrics dict\"\n",
    "    assert 'avg_value' in metrics, \"Missing avg_value in metrics\"\n",
    "    \n",
    "    # Test ReportGenerator\n",
    "    report = reporter.generate_report([(summary, metrics)])\n",
    "    \n",
    "    assert isinstance(report, str), \"Report should be string\"\n",
    "    assert \"ANALYSIS REPORT\" in report, \"Report should contain header\"\n",
    "    assert \"Section 1\" in report, \"Report should contain section\"\n",
    "    \n",
    "    # Test 2: Data validation and cleaning\n",
    "    cleaned_data = validate_and_clean_raw_data([\n",
    "        {'id': 'valid', 'value': 10},\n",
    "        {'value': 20},  # Missing id\n",
    "        {'id': 'invalid'},  # Missing value\n",
    "        'invalid_format'  # Wrong format\n",
    "    ])\n",
    "    \n",
    "    assert isinstance(cleaned_data, list), \"Should return list\"\n",
    "    # Should handle invalid data gracefully\n",
    "    \n",
    "    # Test 3: Integration adapters\n",
    "    test_dict = {'processed_items': [{'processed_value': 10}]}\n",
    "    json_str = dict_to_json_adapter(test_dict)\n",
    "    \n",
    "    assert isinstance(json_str, str), \"Should return JSON string\"\n",
    "    # Should be valid JSON\n",
    "    parsed = json.loads(json_str)\n",
    "    assert parsed == test_dict, \"Should preserve data structure\"\n",
    "    \n",
    "    # Test 4: Full pipeline integration\n",
    "    sample_datasets = [\n",
    "        [{'id': 'A1', 'value': 10}, {'id': 'A2', 'value': 20}],\n",
    "        [{'id': 'B1', 'value': 5}],\n",
    "        []  # Empty dataset\n",
    "    ]\n",
    "    \n",
    "    final_report = integrated_pipeline(sample_datasets)\n",
    "    \n",
    "    assert isinstance(final_report, str), \"Pipeline should return string report\"\n",
    "    assert \"ANALYSIS REPORT\" in final_report, \"Should contain report header\"\n",
    "    \n",
    "    # Should handle multiple sections\n",
    "    assert \"Section 1\" in final_report, \"Should have first section\"\n",
    "    assert \"Section 2\" in final_report, \"Should have second section\"\n",
    "    \n",
    "    # Test 5: Error handling\n",
    "    # Test with invalid input\n",
    "    error_report = integrated_pipeline([])\n",
    "    assert isinstance(error_report, str), \"Should handle empty input gracefully\"\n",
    "    \n",
    "    # Test with malformed data\n",
    "    malformed_report = integrated_pipeline([[\"not\", \"a\", \"dict\", \"list\"]])\n",
    "    assert isinstance(malformed_report, str), \"Should handle malformed data\"\n",
    "    \n",
    "    # Test 6: Edge cases\n",
    "    edge_cases = [\n",
    "        [{'id': 'only_id'}],  # Missing value\n",
    "        [{'value': 42}],      # Missing id\n",
    "        [{}],                 # Empty dict\n",
    "    ]\n",
    "    \n",
    "    edge_report = integrated_pipeline(edge_cases)\n",
    "    assert isinstance(edge_report, str), \"Should handle edge cases\"\n",
    "    assert \"ANALYSIS REPORT\" in edge_report, \"Should still generate report structure\"\n",
    "    \n",
    "    print(\"âœ“ All Question 10 tests passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_question_10()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing integrated pipeline...\n",
      "âœ“ All Question 10 tests passed!\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "id": "d7b4fe67",
   "metadata": {},
   "source": [
    "## Final Submission Instructions\n",
    "\n",
    "### Before You Submit:\n",
    "\n",
    "**Code Quality Checklist:**\n",
    "- All test cells pass without errors\n",
    "- Code follows Python best practices and conventions  \n",
    "- Functions include appropriate documentation\n",
    "- Error handling is implemented where required\n",
    "- Edge cases are handled appropriately\n",
    "- Code is clean, readable, and maintainable\n",
    "\n",
    "**Save Your Work:**\n",
    "- **Save all code outputs** - Run all cells and keep the output visible\n",
    "- Save the notebook file (Ctrl+S / Cmd+S)\n",
    "- Verify all your implementations are in the correct code cells\n",
    "- Double-check that test cells show \"tests passed!\" messages\n",
    "\n",
    "### Submission Format:\n",
    "Submit your completed `firstname_lastname.ipynb` file with **all outputs preserved**. We want to see:\n",
    "- Your code implementations\n",
    "- Test results (passed/failed)\n",
    "- Any debugging output or print statements\n",
    "- Cell execution numbers\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
